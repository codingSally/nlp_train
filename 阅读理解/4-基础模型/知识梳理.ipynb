{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ELMo\n",
    "\n",
    "## 提出背景\n",
    "\n",
    "- 之前2013年提出的word2vec以及2014年提出的Glove, 在构建词向量时，将一个词生成对应的向量，无法解决一次多义的问题，而Elmo提出了一个很好的解决方案，不同于以往的一个词对应一个向量，是固定的，它预训练好的模型不再只是向量对应关系，而是一个预训练好的模型。\n",
    "\n",
    "- 使用时，将一句话或一段话输入模型，模型会根据上下文来推断每个词对应的向量\n",
    "\n",
    "- 这样的好处就是可以结合上下文语境来对多义词进行理解\n",
    "\n",
    "\n",
    "## 基本概念\n",
    "\n",
    "- Elmo的双向lstm语言模型\n",
    "\n",
    "- ELMo是一种新型深度语境化词表征，可对词进行复杂特征(如句法和语义)和词在语言语境中的变化进行建模(即对多义词进行建模)。我们的词向量是深度双向语言模型(biLM)内部状态的函数，在一个大型文本语料库中预训练而成。\n",
    "\n",
    "## 模型架构图\n",
    "\n",
    "![title](img/elmo1.png)\n",
    "\n",
    "- 其中，词向量的表示是基于当前的句子上下文\n",
    "\n",
    "- 高层LSTM, 用于捕捉上下文的词特征（语义）\n",
    "\n",
    "- 底层LSTM，用于捕捉句法层次信息（语法）\n",
    "\n",
    "## 基本原理\n",
    "\n",
    "![title](img/elmo2.png)\n",
    "\n",
    "\n",
    "## 模型优缺点\n",
    "\n",
    "### 优点\n",
    "\n",
    "- 解决了一词多义的问题\n",
    "\n",
    "- 在一些任务上效果有提升\n",
    "\n",
    "![title](img/elmo3.png)\n",
    "\n",
    "### 缺点\n",
    "\n",
    "- LSTM串行，训练成本大\n",
    "\n",
    "- LSTM对于长距离的特征提取，不如LSTM\n",
    "\n",
    "## 相关论文\n",
    "\n",
    "- https://arxiv.org/pdf/1802.05365.pdf\n",
    "\n",
    "\n",
    "## 相关博文\n",
    "\n",
    "- https://www.cnblogs.com/huangyc/p/9860430.html\n",
    "\n",
    "- https://zhuanlan.zhihu.com/p/51679783"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT\n",
    "\n",
    "- Generative Pre-Training\n",
    "\n",
    "## 基本原理\n",
    "\n",
    "- GPT模型分为两阶段，其核心思想是先通过无标签的文本去训练生成语言模型，再根据具体的NLP任务（如文本分类、机器翻译等），来通过有标签的数据对模型进行fine-tuning。\n",
    "\n",
    "- 具体来说，在这篇论文中提出了半监督的方法，即结合了无监督的预训练和有监督的fine-tuning。论文采用两阶段训练。首先，在未标记数据集上训练语言模型来学习神经网络模型的初始参数。随后，使用相应NLP任务中的有标签的数据地将这些参数微调，来适应当前任务。\n",
    "\n",
    "- GPT的预训练过程，其实和ELMO是类似的，主要不同在于两点：\n",
    "\n",
    "> 首先，特征抽取器不是用的RNN，而是用的Transformer，上面提到过它的特征抽取能力要强于RNN\n",
    "\n",
    "> 其次，GPT的预训练虽然仍然是以语言模型作为目标任务，但是采用的是单向的语言模型\n",
    "\n",
    "> - 所谓“单向”的含义是指：语言模型训练的任务目标是根据单词的上下文去正确预测单词Wi, Wj之前的单词序列Context-before称为上文，之后的单词序列Context-after称为下文。ELMO在做语言模型预训练的时候，预测单词Wi同时使用了上文和下文，而GPT则只采用Context-before这个单词的上文来进行预测，而抛开了下文\n",
    "\n",
    "## 模型结构\n",
    "\n",
    "\n",
    "## 论文地址\n",
    "\n",
    "\n",
    "## 相关博文\n",
    "\n",
    "- https://zhuanlan.zhihu.com/p/59286975\n",
    "\n",
    "- https://blog.csdn.net/sinat_24330297/article/details/102501549"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
