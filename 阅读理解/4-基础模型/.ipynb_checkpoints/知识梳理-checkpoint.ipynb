{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ELMo\n",
    "\n",
    "## 提出背景\n",
    "\n",
    "- 之前2013年提出的word2vec以及2014年提出的Glove, 在构建词向量时，将一个词生成对应的向量，无法解决一次多义的问题，而Elmo提出了一个很好的解决方案，不同于以往的一个词对应一个向量，是固定的，它预训练好的模型不再只是向量对应关系，而是一个预训练好的模型。\n",
    "\n",
    "- 使用时，将一句话或一段话输入模型，模型会根据上下文来推断每个词对应的向量\n",
    "\n",
    "- 这样的好处就是可以结合上下文语境来对多义词进行理解\n",
    "\n",
    "\n",
    "## 基本概念\n",
    "\n",
    "- Elmo的双向lstm语言模型\n",
    "\n",
    "- ELMo是一种新型深度语境化词表征，可对词进行复杂特征(如句法和语义)和词在语言语境中的变化进行建模(即对多义词进行建模)。我们的词向量是深度双向语言模型(biLM)内部状态的函数，在一个大型文本语料库中预训练而成。\n",
    "\n",
    "## 模型架构图\n",
    "\n",
    "![title](img/elmo1.png)\n",
    "\n",
    "- 其中，词向量的表示是基于当前的句子上下文\n",
    "\n",
    "- 高层LSTM, 用于捕捉上下文的词特征（语义）\n",
    "\n",
    "- 底层LSTM，用于捕捉句法层次信息（语法）\n",
    "\n",
    "## 基本原理\n",
    "\n",
    "![title](img/elmo2.png)\n",
    "\n",
    "\n",
    "## 模型优缺点\n",
    "\n",
    "### 优点\n",
    "\n",
    "- 解决了一词多义的问题\n",
    "\n",
    "- 在一些任务上效果有提升\n",
    "\n",
    "![title](img/elmo3.png)\n",
    "\n",
    "### 缺点\n",
    "\n",
    "- LSTM串行，训练成本大\n",
    "\n",
    "- LSTM对于长距离的特征提取，不如LSTM\n",
    "\n",
    "## 相关论文\n",
    "\n",
    "- https://arxiv.org/pdf/1802.05365.pdf\n",
    "\n",
    "\n",
    "## 相关博文\n",
    "\n",
    "- https://www.cnblogs.com/huangyc/p/9860430.html\n",
    "\n",
    "- https://zhuanlan.zhihu.com/p/51679783"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT\n",
    "\n",
    "- Generative Pre-Training\n",
    "\n",
    "## 基本原理\n",
    "\n",
    "- GPT模型分为两阶段，其核心思想是先通过无标签的文本去训练生成语言模型，再根据具体的NLP任务（如文本分类、机器翻译等），来通过有标签的数据对模型进行fine-tuning。\n",
    "\n",
    "- 具体来说，在这篇论文中提出了半监督的方法，即结合了无监督的预训练和有监督的fine-tuning。论文采用两阶段训练。首先，在未标记数据集上训练语言模型来学习神经网络模型的初始参数。随后，使用相应NLP任务中的有标签的数据地将这些参数微调，来适应当前任务。\n",
    "\n",
    "- GPT的预训练过程，其实和ELMO是类似的，主要不同在于两点：\n",
    "\n",
    "> 首先，特征抽取器不是用的RNN，而是用的Transformer，上面提到过它的特征抽取能力要强于RNN\n",
    "\n",
    "> 其次，GPT的预训练虽然仍然是以语言模型作为目标任务，但是采用的是单向的语言模型\n",
    "\n",
    "> - 所谓“单向”的含义是指：语言模型训练的任务目标是根据单词的上下文去正确预测单词Wi, Wj之前的单词序列Context-before称为上文，之后的单词序列Context-after称为下文。ELMO在做语言模型预训练的时候，预测单词Wi同时使用了上文和下文，而GPT则只采用Context-before这个单词的上文来进行预测，而抛开了下文\n",
    "\n",
    "- 模型的目标是学习一个通用的语言表示，可以经过很小的调整就应用到各种任务中\n",
    "\n",
    "- 这个模型的设置不需要目标任务和非标注的数据集在同一个领域\n",
    "\n",
    "## 模型结构\n",
    "\n",
    "![title](img/elmo4.png)\n",
    "\n",
    "## 模型原理\n",
    "\n",
    "![title](img/elmo5.png)\n",
    "\n",
    "![title](img/elmo6.png)\n",
    "\n",
    "\n",
    "## 具体任务的微调\n",
    "\n",
    "![title](img/elmo7.png)\n",
    "\n",
    "- GPT的缺点是，单向的语言模型，无法获取上下文相关的特征表示\n",
    "\n",
    "## 论文地址\n",
    "\n",
    "- https://www.cs.ubc.ca/~amuham01/LING530/papers/radford2018improving.pdf\n",
    "\n",
    "\n",
    "## 相关博文\n",
    "\n",
    "- https://zhuanlan.zhihu.com/p/59286975\n",
    "\n",
    "- https://blog.csdn.net/sinat_24330297/article/details/102501549"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT\n",
    "\n",
    "- Bidirectional Encoder Representations from Transformer\n",
    "\n",
    "- 基于语义理解的深度双向预训练Transformer\n",
    "\n",
    "## BERT的贡献\n",
    "\n",
    "- 证明了双向模型对文本特征表示的重要性\n",
    "\n",
    "- 证明了预训练模型能够消除很多繁重的任务相关的网络结构\n",
    "\n",
    "- 在11个NLP任务上，提升了state-of-art水平\n",
    "\n",
    "## 与其它词向量的关系\n",
    "\n",
    "- Word2vec等词向量是词维度，训练好就确定了\n",
    "\n",
    "- BERT句子维度的向量表示，依赖上下文构建结果\n",
    "\n",
    "- 词向量的建立在分布式假设前提下：即，相同上下文语境的词有相似的含义。那么如果两个同义词在不同的上线文中，得到的词向量就是不同的，因此无法处理同义词问题\n",
    "\n",
    "- 而ELMO、GPT、BERT，这些都是结合上下文的动态语言模型表征\n",
    "\n",
    "- 都是无监督训练，不需要标签信息\n",
    "\n",
    "## 背景知识\n",
    "\n",
    "![title](img/bert1.png)\n",
    "\n",
    "\n",
    "![title](img/bert2.png)\n",
    "\n",
    "\n",
    "## 基本概念\n",
    "\n",
    "- 要理解 Bert，5 个关键词帮助理解其思想，分别是 Pre-training、Deep、Bidirectional、Transformer、Language Understanding。\n",
    "\n",
    "> Pre-training: 论文作者认为，确实存在通用的语言模型，先用大量数据预训练一个通用模型，然后再微调模型，使其适用于下游任务。\n",
    "\n",
    "> Deep: Bert 与 Transformer 不同，Bert 的神经网络层更深，意味着它能更准确表达语义的理解，提取更多特征\n",
    "\n",
    "> Bidirectional: Bert 被设计成一个深度双向模型，使得神经网络更有效地从第一层本身一直到最后一层捕获来自目标词的左右上下文信息\n",
    "\n",
    "> Transformer: \n",
    "\n",
    "> - Bert 是基于 Tranformer 的深度双向语言表征模型，也就是利用 Transformer 结构构造了一个多层双向的Encoder 网络。它的特点之一就是所有层都联合上下文语境进行预训练。\n",
    "\n",
    "> - Bert 的目标是生成预训练语言模型，所以只需要 Encoder 机制。Transformer 的 Encoder 是一次性读取整个文本序列，而不是从左到右或者从右到左按顺序读取.\n",
    "\n",
    "> Language Understanding: Bert 是一个语言表征模型，能实现语言表征目标训练，通过深度双向 Transformer 模型达到语义理解的目的。\n",
    "\n",
    "## 模型架构\n",
    "\n",
    "![title](img/bert3.png)\n",
    "\n",
    "\n",
    "- 整体结构： Embedding + Transformer Encoder + Loss优化 \n",
    "\n",
    "## 基本原理\n",
    "\n",
    "###  Embedding\n",
    "\n",
    "- Bert的输入相较\n",
    "\n",
    "### Transformer Encoder\n",
    "\n",
    "\n",
    "###  Loss优化\n",
    "\n",
    "## 论文地址\n",
    "\n",
    "\n",
    "## 相关博文\n",
    "\n",
    "- https://zhuanlan.zhihu.com/p/50913043\n",
    "\n",
    "- https://www.jianshu.com/p/810ca25c4502\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 后续总结三大类预训练语言模型的对比"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
