{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 词向量-Glove\n",
    "\n",
    "## 词向量方法总结\n",
    "\n",
    "### Bow\n",
    "\n",
    "- ont-hot 表示法\n",
    "\n",
    "- TF 表示法\n",
    "\n",
    "- TF-IDF 表示法\n",
    "\n",
    "### Word2Vec\n",
    "\n",
    "- Doc2Vec\n",
    "\n",
    "### Glove\n",
    "\n",
    "\n",
    "### Elmo\n",
    "\n",
    "\n",
    "### BERT\n",
    "\n",
    "\n",
    "## Glove\n",
    "\n",
    "### 基本概念介绍\n",
    "\n",
    "- 全称：Global Vectors for Word Representation\n",
    "\n",
    "- 2014年，斯坦福大学提出的一种新的词矩阵生成的方法\n",
    "\n",
    "- 它既利用了全局的统计信息，也利用了局部的统计信息, 什么意思呢\n",
    "\n",
    "> 利用全局统计信息指的是：它利用全局语料库构建词频矩阵\n",
    "\n",
    "> 利用局部统计信息指的是：在生成词频矩阵时，采用滑动窗口，统计词的共现\n",
    "\n",
    "- Glove是一种语言模型，可以用来生成词向量\n",
    "\n",
    "- 是一个基于全局词频统计的词表征工具，它可以把一个单词表达成一个由实数组成的向量，这些向量捕捉到了单词之间一些语义特性，比如相似性（similarity）、类比性（analogy）等\n",
    "\n",
    "- 融入全局的先验统计信息，可以加快模型的训练速度\n",
    "\n",
    "> 什么叫融入全局的先验统计信息？指的是基于语料构建的词频统计信息，可以预先存储下来，之后再训练模型的时候，直接拿来用。\n",
    "\n",
    "- 通过先验统计信息，可以控制词的相对权重\n",
    "\n",
    "> GloVe根据两个单词在上下文窗口的距离 d，提出了一个衰减函数（decreasing weighting）：decay=1/d用于计算权重，也就是说距离越远的两个单词所占总计数（total count）的权重越小。\n",
    "\n",
    "- 总之，Glove是基于统计的语言模型，用来生成词向量\n",
    "\n",
    "### Glove与LSA的区别\n",
    "\n",
    "![title](img/Glove-LSA.png)\n",
    "\n",
    "### Glove与Word2Vec的区别\n",
    "\n",
    "![title](img/glove-v2c.png)\n",
    "\n",
    "### Glove基本原理\n",
    "\n",
    "- 参见有道云笔记\n",
    "\n",
    "### 预训练模型链接\n",
    "\n",
    "- https://nlp.stanford.edu/projects/glove/\n",
    "\n",
    "### 参考博文\n",
    "\n",
    "- https://www.biaodianfu.com/glove.html\n",
    "\n",
    "- https://blog.csdn.net/coderTC/article/details/73864097\n",
    "\n",
    "- https://zhuanlan.zhihu.com/p/42073620\n",
    "\n",
    "- http://www.fanyeong.com/2018/02/19/glove-in-detail/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction\n",
    "\n",
    "## CNN\n",
    "\n",
    "- 卷积神经网络（Convolutional Neural Network）\n",
    "\n",
    "### 主要解决的问题\n",
    "\n",
    "- 将复杂问题简化，大量参数降维成少量参数，再做处理\n",
    "\n",
    "> 因为图片像素点比较多，而且还需要考虑RGB三通道，如果全维度计算的话，需要的参数是M*N*3.参数量非常大，所以卷积希望可以降维\n",
    "\n",
    "- 保留图像特征，无论是图像做翻转、旋转、或者位置变换，都能有效的识别出来\n",
    "\n",
    "\n",
    "- 参考博文\n",
    "\n",
    "> https://easyai.tech/ai-definition/cnn/\n",
    "\n",
    "### 基本概念\n",
    "\n",
    "- 标准定义\n",
    "\n",
    "> “卷积神经网络”表示在网络中采用称为卷积的数学运算。\n",
    "\n",
    "> 卷积是一种特殊的线性操作\n",
    "\n",
    "> 卷积网络是一种特殊的神经网络，他们在至少一个层中使用卷积代替一般矩阵乘法\n",
    "\n",
    "- 组成部分\n",
    "\n",
    "> CNN由一个或多个卷积层和顶端的全连通层（对应经典的神经网络）组成，同时也包含关联权重和池化层\n",
    "\n",
    "- 实际应用\n",
    "\n",
    "> 是一种可用于处理网格结构的神经网络，如\n",
    "\n",
    "> 1. 图像处理【可以看作是二维的像素网格】\n",
    "\n",
    "> 2. 时序数据处理【可以认为是在时间轴上有规律地采样形成的一维网络】\n",
    "\n",
    "### 三个重要思想\n",
    "\n",
    "- 卷积运算通过三个重要思想来帮助改机机器学习系统：稀疏交互、参数共享和等变表示。\n",
    "\n",
    "#### 稀疏交互\n",
    "\n",
    "- 是指: 不是每个输出单元与输入单元都产生交互\n",
    "\n",
    "![title](img/CNN1.png)\n",
    "\n",
    "> 通过卷积核提取局部特征，而不是每一个像素点的特征，所以不是每个输出单元都与输入单元产生交互\n",
    "\n",
    "#### 参数共享\n",
    "\n",
    "- 是指: 多个函数相同参数\n",
    "\n",
    "![title](img/CNN2.png)\n",
    "\n",
    "> 我理解的是，比如像之前那种全连接网络来说，每一个当前输入需要与下一层的所有输出相连接，从而需要学习的参数就是M*N的关系。而使用了卷积核之后呢，同一个位置对应的多个输入点，可以共享同一份参数【可以通过网格平移想象一下】\n",
    "\n",
    "#### 平移等变\n",
    "\n",
    "- 是指: 输入改变，输出也以同样的方式改变\n",
    "\n",
    "![title](img/CNN3.png)\n",
    "\n",
    "### 基本原理\n",
    "\n",
    "#### CNN组成部分\n",
    "\n",
    "- 输入层：主要负责接收数据\n",
    "\n",
    "> 与传统机器学习一样，送模型之前需要进行预处理，常见的三种预处理方式\n",
    "\n",
    "> 1. 去均值\n",
    "\n",
    "> 2. 归一化\n",
    "\n",
    "> 3. PCA|SVD降维\n",
    "\n",
    "- 卷积层：主要负责提取图像中的局部特征【提取局部特征】\n",
    "\n",
    "> 这个过程我们可以理解为我们使用一个过滤器（卷积核）来过滤图像的各个小区域，从而得到这些小区域的特征值。\n",
    "\n",
    "> 在具体应用中，往往有多个卷积核，可以认为，每个卷积核代表了一种图像模式，如果某个图像块与此卷积核卷积出的值大，则认为此图像块十分接近于此卷积核(每个卷积核代表一种模式)。如果我们设计了6个卷积核，可以理解：我们认为这个图像上有6种底层纹理模式，也就是我们用6中基础模式就能描绘出一副图像。\n",
    "\n",
    "- 激活层：对卷积层的输出，做一次非线性映射【非线性变换】\n",
    "\n",
    "> 常见的激活函数：Sigmoid函数、Tanh函数、ReLU、Leaky ReLU、ELU、Maxout\n",
    "\n",
    "> 激活函数使用建议：首先ReLU，因为迭代速度快，但是有可能效果不佳。如果ReLU失效的情况下，考虑使用Leaky ReLU或者Maxout，此时一般情况都可以解决。Tanh函数在文本和音频处理有比较好的效果。\n",
    "\n",
    "- 池化层：用来大幅降低参数两集【降维、避免过拟合、提高模型容错性】\n",
    "\n",
    "> 池化层简单理解就是下采样，它可以降低数据维度，避免过拟合，同时可以压缩数据和参数的数量以及提高模型的容错性。\n",
    "\n",
    "> 之所以这么做的原因，是因为即使做完了卷积，图像仍然很大（因为卷积核比较小），所以为了降低数据维度，就进行下采样。\n",
    "\n",
    "> 池化层主要有最大池化层和平均池化层\n",
    "\n",
    "- 全连接层：类似传统神经网络的部分，用来输出想要的结果【输出结果】\n",
    "\n",
    "> 经过卷积层和池化层降维过的数据，全连接层才能”跑得动”，不然数据量太大，计算成本高，效率低下。\n",
    "\n",
    "> 在全连接层之前，如果神经元数目过大，学习能力强，有可能出现过拟合。因此，可以引入dropout操作，来随机删除神经网络中的部分神经元.【dropout的工作原理后面需要梳理】\n",
    "\n",
    "> 还可以进行局部归一化（LRN）、数据增强等操作，来增加鲁棒性.\n",
    "\n",
    "- Dropout【避免过拟合】\n",
    "\n",
    "> 一定概率关闭神经元，避免过拟合\n",
    "\n",
    "> 原理待补充\n",
    "\n",
    "### 优缺点\n",
    "\n",
    "#### 优点\n",
    "\n",
    "- 共享卷积核（共享参数）、优化计算量、可处理高维数据\n",
    "\n",
    "- 无需手动选择特征，只要训练好权重，就可得到特征\n",
    "\n",
    "- 深层次抽取信息丰富，表达效果好\n",
    "\n",
    "#### 缺点\n",
    "\n",
    "- 需要调参、需要大量训练样本\n",
    "\n",
    "- 训练迭代次数比较多，需要GPU资源\n",
    "\n",
    "- 难以直观解释\n",
    "\n",
    "### TextCNN\n",
    "\n",
    "#### 基本概念\n",
    "\n",
    "- TextCNN是卷积神经网络的一种\n",
    "\n",
    "- 与传统图像的CNN网络相比, textCNN 在网络结构上没有任何变化(甚至更加简单了), textCNN只有一层卷积,一层max-pooling, 最后将输出外接softmax 来n分类。\n",
    "\n",
    "- 将卷积神经网络CNN应用到文本分类任务，利用多个不同size的kernel来提取句子中的关键信息（类似于多窗口大小的ngram），从而能够更好地捕捉局部相关性。\n",
    "\n",
    "#### 网络结构\n",
    "\n",
    "![title](img/TextCNN1.png)\n",
    "\n",
    "\n",
    "![title](img/TextCNN3.png)\n",
    "\n",
    "#### 原理图\n",
    "\n",
    "![title](img/TextCNN2.png)\n",
    "\n",
    "![title](img/TextCNN11.png)\n",
    "\n",
    "- 1. CNN中卷积层中的每一个叫做卷积核，而不是卷积，卷积是一种运算\n",
    "\n",
    "- 2. 原理图中有3个不同的区域大小[卷积核的大小]，分别是 n=2、3、4， 类似于ngram中的n= 2、3、4,即和最近多少个词有关系\n",
    "\n",
    "- 3. filter的理解：\n",
    "\n",
    "> 在有的文档中，一个filter等同于一个卷积核：只是指定了卷积核的长宽深；\n",
    "\n",
    "> 而有的情况（例如tensorflow等框架中，filter参数通常指定了卷积核的长、宽、深、个数四个参数），filter包含了卷积核形状和卷积核数量的概念：即filter既指定了卷积核的长宽深，也指定了卷积核的数量。\n",
    "\n",
    "![title](img/TextCNN4.png)\n",
    "\n",
    "> 所以这里每种类型的卷积核2，3，4是为了模拟类似n元语法的思想。而每种卷积核又有不同的filter，如word2vec、Glove是为了多元化词向量，最终的目标都是为了捕捉更多的不同特征。\n",
    "\n",
    "> filter初始值是随机初始化的，参数值是通过训练不断更新的。\n",
    "\n",
    "- 4. 所以原理图中，并没有指出是6个卷积核还是3个，但是指出了卷积核的类别(3种)，所以可以按自己理解就行\n",
    "\n",
    "- 5. channel的理解：\n",
    "\n",
    "> 每个卷积层中卷积核的数量, 在这里就是6\n",
    "\n",
    "![title](img/TextCNN5.png)\n",
    "\n",
    "\n",
    "![title](img/TextCNN7.png)\n",
    "\n",
    "> 原文中，每个卷积核类型，采用双通道\n",
    "\n",
    "- 6. 原文中使用了L2正则化项 \n",
    "\n",
    "- 7. 所以在这个原理图中，就是3种类型的卷积核（n=2,3,4）,6个过滤器，整个卷积层有6个channels.\n",
    "\n",
    "#### 输入层设计\n",
    "\n",
    "- CNN-rand(单channel)\n",
    "> 设计好 embedding_size 这个超参数后, 对不同单词的向量作随机初始化, 后续BP的时候作调整\n",
    "\n",
    "- CNN-Static(单channel)\n",
    "> 拿 pre-trained vectors from word2vec, FastText or GloVe 直接用, 训练过程中不再调整词向量. 这也算是迁移学习的一种思想\n",
    "\n",
    "- CNN-non-static(单channel)\n",
    "> pre-trained vectors + fine tuning , 即拿word2vec训练好的词向量初始化, 训练过程中再对它们微调\n",
    "\n",
    "- CNN-multiple channel(多channels)\n",
    "> 类比于图像中的RGB通道, 这里也可以用 static 与 non-static 搭两个通道来搞.\n",
    "\n",
    "输入句子长度不一致怎么办：补0\n",
    "\n",
    "\n",
    "![title](img/TextCNN10.png)\n",
    "\n",
    "#### 超参数\n",
    "\n",
    "- sequence_length：定长处理, 超过的截断, 不足的补0\n",
    "\n",
    "- num_classes：多分类, 分为几类\n",
    "\n",
    "- vocabulary_size：语料库的词典大小\n",
    "\n",
    "- embedding_size：词向量的维度\n",
    "\n",
    "- filter_size ：一般是个array, 输入多个filter\n",
    "\n",
    "\n",
    "#### 实验的结论\n",
    "\n",
    "- 预训练word2vec 或GloVe效果好于onehot\n",
    "\n",
    "- 卷积核大小：有较大影响，可取1~10，越长文本越大。\n",
    "\n",
    "- 卷积核的数量：有较大的影响，一般取100~600 ，一般可使用Dropout（0~0.5）。\n",
    "\n",
    "- 选用ReLU 和 tanh作为激活函数\n",
    "\n",
    "- 使用1-max pooling\n",
    "\n",
    "- 当增加的feature map导致性能降低时，例如：大于0.5的Dropout。\n",
    "\n",
    "- 使用交叉验证，评估模型性能\n",
    "\n",
    "- 使用预训练模型效果比较好\n",
    "\n",
    "![title](img/TextCNN8.png)\n",
    "\n",
    "- 使用多通道没有达到预期的效果【预期想避免过拟合（通过正则化就可以实现）】\n",
    "\n",
    "![title](img/TextCNN9.png)\n",
    "\n",
    "#### 论文地址\n",
    "\n",
    "- https://arxiv.org/pdf/1408.5882.pdf\n",
    "\n",
    "- https://arxiv.org/pdf/1510.03820.pdf\n",
    "\n",
    "\n",
    "#### 参考博文\n",
    "\n",
    "> https://www.cnblogs.com/bymo/p/9675654.html\n",
    "\n",
    "> https://www.cnblogs.com/ModifyRong/p/11319301.html\n",
    "\n",
    "### 背景知识\n",
    "\n",
    "- 什么是卷积运算\n",
    "\n",
    "> 卷积是一种积分运算，用来求两个曲线重叠区域面积\n",
    "\n",
    "> 可以看做加权求和，可以用来消除噪声、特征增强\n",
    "\n",
    "> 通过卷积把一个点的像素值用它周围的点的像素值的加权平均代替\n",
    "\n",
    "> 卷积在深度学习中，可以简单的理解为“加权求和”\n",
    "\n",
    "- 计算公式\n",
    "\n",
    "> 离散变量卷积运算公式\n",
    "\n",
    "![title](img/CNN4.png)\n",
    "\n",
    "> 连续变量卷积运算公式\n",
    "\n",
    "![title](img/CNN5.png)\n",
    "\n",
    "- 计算方式\n",
    "\n",
    "> 注意和矩阵运算方式有点不同，卷积是倒着计算的【但是在神经网络中还是正着计算的（所以，实际上，在神经网络中，不是严格的卷积运算，而是一种互预算 -- 直白理解就是对应元素相乘之后加和）】\n",
    "\n",
    "- 参考博文\n",
    "\n",
    "> zhihu.com/question/22298352\n",
    "\n",
    "> https://www.zhihu.com/question/49376084\n",
    "\n",
    "> https://www.zhihu.com/question/49376084\n",
    "\n",
    "### 参考博文\n",
    "\n",
    "- https://blog.csdn.net/Daycym/article/details/90140124\n",
    "\n",
    "- https://blog.csdn.net/weixin_42813521/article/details/104991490\n",
    "\n",
    "- https://zhuanlan.zhihu.com/p/48134104"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Context-Query Interaction\n",
    "\n",
    "## 注意力机制\n",
    "\n",
    "### 背景介绍\n",
    "\n",
    "- 思想来源于人类的视觉注意力机制，即人类在观察一个东西时，不会直接就观察整体，而是根据需要观察某个相关的、重要的部分。\n",
    "\n",
    "- Attention 概念最早在图像领域出现\n",
    "\n",
    "### 基本概念\n",
    "\n",
    "- Attention是一种思想，而不是框架，它主要想通过某种方式，赋予模型对于重要性的区分和辨别能力。其本质上是一系列权重的分配。\n",
    "\n",
    "### Seq2Seq\n",
    "\n",
    "- Encoder-Decoder框架简介\n",
    "\n",
    "> 要了解深度学习中的注意力模型，就必须要知道Encoder-Decoder框架，因为目前大多数注意力机制都附着在Encoder-Decoder框架下。\n",
    "\n",
    "> 当然，我们也应该明白注意力机制是一种思想，本身并不依赖于任何框架。\n",
    "\n",
    "![title](img/S2S1.png)\n",
    "\n",
    "- Encoder-Decoder框架结构\n",
    "\n",
    "![title](img/S2S2.png)\n",
    "\n",
    "- 这种架构存在的弊端\n",
    "\n",
    "![title](img/S2S3.png)\n",
    "\n",
    "> 基础的Encoder-Decoder框架是没有体现“注意力模型”的，所以可以把它看做注意力不集中的分心模型【即他认为源句子中的任意单词对生成某个目标单词来说，影响力是一样的，所以说它是没有体现注意力的（可以想象一下，就像是我们盯着一幅画，眼神却没有聚焦，而是比较散漫）】。\n",
    "\n",
    "> 没有引入注意力的模型在输入句子比较短的时候问题不大，但是如果输入句子比较长，此时所有语义完全通过一个中间语义向量来表示，单词自身的信息已经消失，可想而知会丢失很多细节信息.\n",
    "\n",
    "- 为什么说没有引入注意力机制\n",
    "\n",
    "![title](img/S2S7.png)\n",
    "\n",
    "- 举例说明\n",
    "\n",
    "> 以LSTM-LSTM框架为例, LSTM-1 对输入序列学习，编码为固定长度，LSTM-2 单元解码输出序列。\n",
    "\n",
    "![title](img/S2S6.png)\n",
    "\n",
    "> 其存在缺点\n",
    "\n",
    "> 1. 固定长度的向量表示，限制了模型的性能\n",
    "\n",
    "> 2. 输入序列比较长时，模型的性能会变得很差\n",
    "\n",
    "> 3. 难以保留全部的必要信息\n",
    "\n",
    "- 论文地址\n",
    "\n",
    "> https://papers.nips.cc/paper/2014/file/a14ac55a4f27472c5d894ec1c3c743d2-Paper.pdf\n",
    "\n",
    "### Attention\n",
    "\n",
    "- 为了解决Encoder-Decoder框架中的两个弊端，Bahdanau等人在论文《Neural Machine Translation by Jointly Learning to Align and Translate》中提出使用Attention机制。\n",
    "\n",
    "#### Attention基本原理\n",
    "\n",
    "- 基本思想\n",
    "\n",
    "![title](img/S2S5.png)\n",
    "\n",
    "- 原理图\n",
    "\n",
    "![title](img/S2S4.png)\n",
    "\n",
    "- 如何理解引入注意力机制\n",
    "\n",
    "![title](img/S2S8.png)\n",
    "\n",
    "![title](img/S2S9.png)\n",
    "\n",
    "- 如何得到每个单词的概率分布\n",
    "\n",
    "![title](img/S2S10.png)\n",
    "\n",
    "![title](img/S2S11.png)\n",
    "\n",
    "![title](img/S2S12.png)\n",
    "\n",
    "> 直观解释就是：通过上一时刻隐藏状态的输出，和输入中的每个单词作比较（即通过某种函数F计算当前隐藏层的输出和输入的哪个单词，配对的可能性更高，就赋予更高的权重），\n",
    "\n",
    "> 怎么理解Attention的物理含义呢？在NLP中可以理解为输出句子中某个单词和输入句子中每个单词的配对模型（目标句子生成的每个单词对应输入句子单词的概率分布可以理解为输入句子单词和这个目标生成单词的配对概率）\n",
    "\n",
    "- 本质思想\n",
    "\n",
    "![title](img/S2S13.png)\n",
    "\n",
    "> 从概念上理解，可以把Attention理解为从大量信息中有选择地筛选出少量重要信息并聚焦到这些重要信息上，忽略大多不重要的信息。\n",
    "> 聚焦的过程体现在权重系数的计算上，权重越大越聚焦于其对应的Value值上，即权重代表了信息的重要性，而Value是其对应的信息。\n",
    "\n",
    "> 直白理解：可以这样理解这里的Key就是上一时刻隐藏状态的输出，Query就是目标单词，而Value就是F(输入单词)【其中F就是通过某种函数对输入单词做的变换】。所以整体过程可以理解为，通过计算目标单词和上一时刻隐藏状态输出值的相似性【配对可能性】，计算出配对概率分布，之后将这个概率分布值作用在F(输入单词)上后，加权求和即得到对应的Attention。\n",
    "\n",
    "- 计算过程 \n",
    "\n",
    "> 如果对目前大多数方法进行抽象的话，可以将其归纳为两个过程：第一个过程是根据Query和Key计算权重系数，第二个过程根据权重系数对Value进行加权求和。而第一个过程又可以细分为两个阶段：第一个阶段根据Query和Key计算两者的相似性或者相关性；第二个阶段对第一阶段的原始分值进行归一化处理；\n",
    "\n",
    "![title](img/S2S14.png)\n",
    "\n",
    "> 计算步骤总结\n",
    "\n",
    "> - query和key进行相似度计算，得到权值\n",
    "\n",
    "> - 将权值进行归一化，得到直接可用的权重\n",
    "\n",
    "> - 将权重和value进行加权求和\n",
    "\n",
    "> 其中，在第一个阶段，可以引入不同的函数和计算机制，根据Query和某个Key_i，计算两者的相似性或者相关性\n",
    "\n",
    "> 最常见的方法包括：求两者的向量点积、求两者的向量Cosine相似性或者通过再引入额外的神经网络来求值\n",
    "\n",
    "![title](img/S2S16.png)\n",
    "\n",
    "> 第一阶段产生的分值根据具体产生的方法不同,其数值取值范围也不一样，第二阶段引入类似SoftMax的计算方式对第一阶段的得分进行数值转换.\n",
    "> - 一方面可以进行归一化，将原始计算分值整理成所有元素权重之和为1的概率分布\n",
    "\n",
    "> - 另一方面也可以通过SoftMax的内在机制更加突出重要元素的权重\n",
    "\n",
    "> - 一般采用如下计算公式\n",
    "\n",
    "![title](img/S2S15.png)\n",
    "\n",
    "> 第二阶段的计算结果a_i即为value_i对应的权重系数，然后进行加权求和即可得到Attention数值,计算公式如下\n",
    "\n",
    "![title](img/S2S17.png)\n",
    "\n",
    "\n",
    "> 通过如上三个阶段的计算，即可求出针对Query的Attention数值，目前绝大多数具体的注意力机制计算方法都符合上述的三阶段抽象计算过程\n",
    "\n",
    "- Attention类型\n",
    "\n",
    "> Soft Attention\n",
    "\n",
    "> - 这是比较常见的Attention方式，对所有key求权重概率，每个key都有一个对应的权重，是一种全局的计算方式（也可以叫Global Attention）\n",
    "\n",
    "> - 这种方式比较理性，参考了所有key的内容，再进行加权。但是计算量会比较大一些，尤其当encoder句子偏长，效率偏低。\n",
    "\n",
    "> Hard Attention\n",
    "\n",
    "> - 这种方式是直接精准定位到某个key，其余key就都不管了，相当于这个key的概率是1，其余key的概率全部是0。\n",
    "\n",
    "> - 因此这种对齐方式要求很高，要求一步到位，如果没有正确对齐，会带来很大的影响。另一方面，因为不可导，一般需要用强化学习的方法进行训练。（或者使用gumbel softmax之类的）\n",
    "\n",
    "> Local Attention\n",
    "\n",
    "> - 这种方式其实是以上两种方式的一个折中，对一个窗口区域进行计算。先用Hard方式定位到某个地方，以这个点为中心可以得到一个窗口区域，在这个小区域内用Soft方式来算Attention。\n",
    "\n",
    "- 相关博文\n",
    "\n",
    "> https://www.cnblogs.com/jins-note/p/13056604.html\n",
    "\n",
    "> https://zhuanlan.zhihu.com/p/35739040\n",
    "\n",
    "> http://vandergoten.ai/2018-09-18-attention-is-all-you-need/\n",
    "\n",
    "> https://zhuanlan.zhihu.com/p/53036028\n",
    "\n",
    "> http://kakuguo.ink/2020-03-18-Summary-of-Computer-Vision-Attention/\n",
    "\n",
    "\n",
    "## Transformer\n",
    "\n",
    "### 模型简介\n",
    "\n",
    "- Transformer中抛弃了传统的CNN和RNN，整个网络结构完全是由Attention机制组成\n",
    "\n",
    "- Transformer由且仅由self-Attenion和Feed Forward Neural Network组成\n",
    "\n",
    "- Transformer的本质上是一个Encoder-Decoder的结构\n",
    "\n",
    "- Transform和LSTM的最大区别，就是LSTM的训练是迭代的，是一个接一个字的来，当前这个字过完LSTM单元，才可以进下一个字。而Transformer的训练是并行的，即所有字同时训练，这大大加快了计算效率。\n",
    "\n",
    "- Transformer模型主要分为两部分，编码器和解码器\n",
    "\n",
    "> 编码器负责把自然语言序列映射成为隐藏层，含有自然语言序列的数学表达\n",
    "\n",
    "> 解码器把隐藏层再映射为自然语言序列\n",
    "\n",
    "- Transformer 可以用于情感分类、命名实体识别、语义关系抽取、摘要生成、机器翻译等\n",
    "\n",
    "### 模型架构图\n",
    "\n",
    "![title](img/trans1.png)\n",
    "\n",
    "- Transformer是可以堆叠的（图中的N× 就是堆叠N个的意思），其思路是越高层的Transformer能够学习到越高级更抽象的信息（类似CNN的堆叠）\n",
    "\n",
    "### 模型原理\n",
    "\n",
    "#### 编码器\n",
    "\n",
    "![title](img/trans2.png)\n",
    "\n",
    "#### 位置嵌入\n",
    "\n",
    "- Transformer是以字作为输入【即一般以字为单位训练Transformer模型】，将字进行字嵌入之后，再与位置嵌入进行相加（不是拼接，就是单纯的对应位置上的数值进行加和）\n",
    "\n",
    "- 为什么需要位置嵌入呢？\n",
    "\n",
    "> 因为Transformer摈弃了RNN的结构，因此需要一个东西来标记各个字之间的时序or位置关系，而这个东西，就是位置嵌入。\n",
    "\n",
    "- 设计思路\n",
    "\n",
    "![title](img/trans3.png)\n",
    "\n",
    "![title](img/trans4.png)\n",
    "\n",
    "- 词向量的最终表达形式\n",
    "\n",
    "![title](img/trans5.png)\n",
    "\n",
    "> 最后向量是每个位置sin和cos结果的拼接 \n",
    "\n",
    "> 模型最终的输入是位置向量和字向量的加和,即\n",
    "\n",
    "![title](img/trans10.png)\n",
    "\n",
    "- 参考博文\n",
    "\n",
    "https://cloud.tencent.com/developer/article/1657630\n",
    "\n",
    "https://kazemnejad.com/blog/transformer_architecture_positional_encoding/ 【这篇文章对位置嵌入讲的比较详细】\n",
    "\n",
    "#### Multi-Head Attention\n",
    "\n",
    "- 背景知识\n",
    "\n",
    "> self attention mechanism, 自注意力机制\n",
    "\n",
    "> 注意力矩阵的作用就是一个注意力权重的概率分布\n",
    "\n",
    "> 点积的物理意义：如果两个向量之间夹角越小（越相似），点积就越大，否则越小\n",
    "\n",
    "> 利用这些Attention score就可以得到一个加权的表示，然后再放到一个前馈神经网络中得到新的表示，这一表示很好的考虑到上下文的信息。Encoder读入输入数据，利用层层叠加的Self-Attention机制对每一个词得到新的考虑了上下文信息的表征。Decoder也利用类似的Self-Attention机制，但它不仅仅看之前产生的输出的文字，而且还要attend encoder的输出\n",
    "\n",
    "- 基本原理\n",
    "\n",
    "> 参考文档《Transformer》中self attention mechanism部分\n",
    "\n",
    "- Multi-Head Attention 的意义\n",
    "\n",
    "> Multi-head Attention其实就是多个Self-Attention结构的结合，每个head学习到在不同表示空间中的特征.两个head学习到的Attention侧重点可能略有不同，这样给了模型更大的容量。\n",
    "\n",
    "> 将模型分为多个头，形成多个子空间，可以让模型去关注不同方面的信息，从而提取多重语义\n",
    "\n",
    "![title](img/trans6.png)\n",
    "\n",
    "> 相当于就是把不同参数的头，作用在相同的输入上，分别提取不同的信息\n",
    "\n",
    "#### Attention Mask\n",
    "\n",
    "- 基本思想\n",
    "\n",
    "> 在self-attention的计算过程中，我们通常使用mini-batch来计算，也就是一次计算多句话，而这多条句子长度不一致，所以需要按照最大长度对剩余句子进行补齐，一般补0，但是这回导致指数运算后，值为1.意味着让无效数据参与了运算，这会存在问题、\n",
    "\n",
    "> 所以，一般就需要做一个mask,让这些无效区域不参与运算\n",
    "\n",
    "> 一般的做法是，给这些无效区域增加一个很大的负数偏置，这样在进行指数运算时，结果就接近于0\n",
    "\n",
    "- 详细解释\n",
    "\n",
    "> 参考文档《Transformer》中Attention Mask部分\n",
    "\n",
    "#### Add & Norm\n",
    "\n",
    "- 背景知识\n",
    "\n",
    "> 误差&残差\n",
    "\n",
    "> - 误差：误差是观察值与真实值之间的差\n",
    "\n",
    "> - 残差: 观察值与模型估计值之间的差\n",
    "\n",
    "- ADD\n",
    "\n",
    "> 就是把原始字嵌入结果与Multi-Head Attention结果做残差链接（说白了就是两个结果都不能100%确定是真实值，所以叫做残差连接）\n",
    "\n",
    "- Norm\n",
    "\n",
    "> 就是把神经网络中隐藏层归一为标准正态分布，以加快训练速度，加速收敛速度\n",
    "\n",
    "- 详细解释\n",
    "\n",
    "> 参考文档《Transformer》中Add & Norm部分\n",
    "\n",
    "\n",
    "#### FeedForward\n",
    "\n",
    "- 基本原理\n",
    "\n",
    "> 将低维空间中，不好分类的数据，映射到高维空间，便于分类【类似于SVM将低维空间中线性不可分数据映射到高维空间中，变的线性可分】\n",
    "\n",
    "![title](img/trans9.png)\n",
    "\n",
    "- 运算方式\n",
    "\n",
    "> 就是基于$X_{attention}$做两层线性变换，之后用激活函数激活，即\n",
    "\n",
    "> $X_{hidden}= Activate(Linear(Linear(X_{attention}))) $\n",
    "\n",
    "### 模型整体运算过程总结\n",
    "\n",
    "![title](img/trans7.png)\n",
    "\n",
    "![title](img/trans8.png)\n",
    "\n",
    "### 优缺点\n",
    "\n",
    "#### 优点\n",
    "\n",
    "- 不对数据的时间和空间关系做假设，可以处理一组对象\n",
    "\n",
    "- 层输出可以并行计算，不像RNN需要序列计算\n",
    "\n",
    "- 远距离项可以影响彼此输出，无需多步RNN或各种卷积层\n",
    "\n",
    "- 可以学习长距离的依赖\n",
    "\n",
    "\n",
    "#### 缺点\n",
    "\n",
    "- 如果输入数据有时间或空间的关系，则必须加上位置编码，否则模型会有效地看到一堆单词\n",
    "\n",
    "\n",
    "### 论文地址\n",
    "\n",
    "- https://arxiv.org/abs/1706.03762\n",
    "\n",
    "- http://jalammar.github.io/illustrated-transformer/\n",
    "\n",
    "### 相关博文\n",
    "\n",
    "- https://zhuanlan.zhihu.com/p/48508221\n",
    "\n",
    "- https://zhuanlan.zhihu.com/p/47510705 【分别介绍了每层】\n",
    "\n",
    "- https://cloud.tencent.com/developer/article/1451467\n",
    "\n",
    "- https://terrifyzhao.github.io/2019/01/11/Transformer%E6%A8%A1%E5%9E%8B%E8%AF%A6%E8%A7%A3.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 阅读理解模型\n",
    "\n",
    "## Attentive Reader & Impatient Reader\n",
    "\n",
    "- 机器阅读理解开山之作，是一款经典的机器阅读理解的神经网络模型\n",
    "\n",
    "- 机器阅读理解（Machine Reading Comprehension）为自然语言处理的核心任务之一，也是评价模型理解文本能力的一项重要任务，其本质可以看作是一种句子关系匹配任务，其具体的预测结果与具体任务有关\n",
    "\n",
    "\n",
    "### 相关背景\n",
    "\n",
    "- 论文作者提出的模型有三个：The Deep LSTM Reader、The Attentive Reader 和 The Impatient Reader。最主要的贡献还是 Attentive Reader 和 Impatient Reader 这两个模型，这两个模型也是机器阅读理解一维匹配模型和二维匹配模型的开山鼻祖。\n",
    "\n",
    "- 构建了问题与数据集\n",
    "\n",
    "- 适用于完形填空任务\n",
    "\n",
    "### 问题构建\n",
    "\n",
    "- 数据集\n",
    "\n",
    "> CNN & Daily Mail\n",
    "\n",
    "- 命名实体NE替换的数据集构建\n",
    "\n",
    "> 命名实体替换技巧\n",
    "\n",
    "> - 每个样本的答案（answer）的条件概率p(a|c,q)\n",
    "\n",
    "> - 将NE替换，可以让模型更关注从上下文挖掘实体的语义关系（因为在不看原文仅通过阅读分析question的情况下，也可以回答question）；但是为了防止训练过程中网络模型过度关注替换后的命名实体，命名实体替换后会进行随机排序，防止训练过程中网络模型过度关注替换后的NE。\n",
    "\n",
    "- CNN&Dailymail语料库的特点\n",
    "\n",
    "> 答案是某种实体对象；答案一定在原文中。因此该语料库不适合用于训练回答推理性的问题。\n",
    "\n",
    "- 非神经网络模型的方法\n",
    "\n",
    "> - 符号匹配模型\n",
    "\n",
    "> 识别句子谓语以及它们的主语和宾语，匹配“谁对谁做了什么事情”的框架来获取信息，如(e1, V, e2)\n",
    "\n",
    "> - 单词距离基准法(word distance)\n",
    "\n",
    "> 将答案占位符 与 上下文文档中每个可能ne对比，计算问题与指定实体上下文之间的距离，选取距离最小的实体对象作为问题答案\n",
    "\n",
    "- 神经网络方法\n",
    "\n",
    "> 这篇文章（https://www.cnblogs.com/sandwichnlp/p/11811396.html）里介绍了多种方法，暂时只梳理Attentive Reader & Impatient Reader\n",
    "\n",
    "\n",
    "### 模型架构\n",
    "\n",
    "![title](img/ar&ir1.png)\n",
    "\n",
    "\n",
    "### Attentive Reader\n",
    "\n",
    "- AR采用注意力机制来构建token级别的网络模型。Attentive Reader将query作为一个整体来分析document中不同token（单词）的注意力。\n",
    "\n",
    "- 原理图\n",
    "\n",
    "![title](img/ar&ir2.png)\n",
    "\n",
    "- 原理介绍\n",
    "\n",
    "![title](img/ar&ir3.png)\n",
    "\n",
    "### Impatient Reader\n",
    "\n",
    "- query中不同token本身的重要性是不一样的。Impatient Reader进一步分析query中的每个token，尝试找到query中token与document中哪几个token关联最大，并且对于query中每个token，都需要考虑到上一个token在document中累积的信息。\n",
    "\n",
    "- 原理图\n",
    "\n",
    "![title](img/ar&ir4.png)\n",
    "\n",
    "- 原理介绍\n",
    "\n",
    "![title](img/ar&ir5.png)\n",
    "\n",
    "> Impatient Reader较Attentive Reader更为复杂，在某些情况下，IR效果可能并不好，因为每读取query中的一个token就要通读原文一次！并且还要考虑上一个token在原文中的相关token，这样效率可能不高，且可能存在梯度弥散问题。\n",
    "\n",
    "### 一维、二维匹配模型区别\n",
    "\n",
    "- 一维匹配模型：将问题直接编码为一个固定长度的向量，在计算注意力分数的时候，等效于直接计算文档D每个词在特点问题上下向量中作为答案的概率：P(a|c,q)，也正是在计算问题向量Q与文档各个词的匹配关系中形成的一维线性结构，称为一维匹配模型.\n",
    "\n",
    "> 概括：问题编码固定长度的向量，计算文档 D 每个词在特定问题上下文向量中作为答案的概率；\n",
    "\n",
    "> 一维匹配模型的注意力分数等效于直接文档 d 中每个词在特定问题上下文向量中作为答案的概\n",
    "\n",
    "- 二维匹配模型：直接输出问题Q中每一个词的编码，计算注意力的时候，计算文档Q中每一词对D中每一个词的注意力，即形成了一个词-词的二维匹配结构。由于二维匹配模型将问题由整体表达语义的一维结构转换成为按照问题中每个单词及其上下文的语义的二维结构，明确引入了更多细节信息，所以整体而言模型效果要稍优于一维匹配模型。\n",
    "\n",
    "> 概括：问题 Q 每一个词编码，计算文档 Q 中每一个词对 D 中每一个词的注意力，形成词 - 词的二维匹配结构，模型效果要稍优于一维匹配模型。\n",
    "\n",
    "\n",
    "### 论文地址\n",
    "\n",
    "- https://proceedings.neurips.cc/paper/2015/file/afdec7005cc9f14302cd0474fd0f3c96-Paper.pdf\n",
    "\n",
    "\n",
    "### 相关博文\n",
    "\n",
    "- https://blog.csdn.net/changreal/article/details/103757471\n",
    "\n",
    "- https://www.cnblogs.com/sandwichnlp/p/11811396.html 【介绍了阅读理解的各种模型】\n",
    "\n",
    "\n",
    "## Bi-DAF\n",
    "\n",
    "- Bidirectional Attention Flow for Machine Comprehension\n",
    "\n",
    "### 基本概念\n",
    "\n",
    "- 正式确立编码层-交互层-输出层的结构\n",
    "\n",
    "- 采用了双向注意力流机制\n",
    "\n",
    "- BiDAF采用多阶段的、层次化的处理，使得模型可以捕获原文不同粒度的特征\n",
    "\n",
    "- 同时使用双向的attention流机制以在without early summarization的情况下获得相关问句和原文之间的表征\n",
    "\n",
    "### 模型结构\n",
    "\n",
    "![title](img/bidaf1.png)\n",
    "\n",
    "- 模型是一个分阶段的多层过程，由6层网络组成【具体见基本原理部分】\n",
    "\n",
    "### 基本原理\n",
    "\n",
    "#### 编码层\n",
    "\n",
    "- Character Embedding Layer【字符嵌入层】\n",
    "\n",
    "> 用字符级CNNs,将每个字映射到向量空间\n",
    "\n",
    "> 字符嵌入是将词映射为一个高纬的向量空间。我们对每个词用CNN处理以获得字符嵌入。每个词由多个字符组成，则视为1D，将其输入到CNN。CNN的输出经过最大池化操作后即可获得每个词对应的字符级向量。\n",
    "\n",
    "> Char CNN 论文地址\n",
    "\n",
    "> - https://papers.nips.cc/paper/2015/file/250cf8b51c773f3f8dc8b4be867a9a02-Paper.pdf\n",
    "\n",
    "- Word Embedding Layer【字嵌入层】\n",
    "\n",
    "> 利用预训练的词嵌入模型(比如Glove),将每个字映射到向量空间\n",
    "\n",
    "> 将字符嵌入和词嵌入拼接后，输入到一个双层的 Highway Network，输出是两个d维度的向量。\n",
    "\n",
    "> Highway Network\n",
    "\n",
    "> - Highway网络，就是输入某一层网络的数据一部分经过非线性变换，另一部分直接从该网络跨过去不做任何转换，而多少的数据需要非线性变换，多少的数据可以直接跨过去，是由一个权值矩阵和输入数据共同决定的。\n",
    "\n",
    "> - 受lstm门机制的启发，Highway network基于门机制引入了transform gate T 和carry gate C\n",
    "\n",
    "> - Highway Network论文地址：\n",
    "\n",
    "> - https://github.com/zsweet/blog_code/blob/master/papers/Machine_comprehesion_using_match_LSTM_and_answer_pointer.pdf\n",
    "\n",
    "- Contextual Embedding Layer【上下文嵌入层】\n",
    "\n",
    "> 利用周围单词的上下文线索来细化单词的嵌入\n",
    "\n",
    "> 这层采用的是LSTM以建模单词之间的交互特征。\n",
    "\n",
    "> 文中采用的是双向LSTM，将两个方向的LSTM结果进行拼接，于是可以从原文词向量 $X$ 得到对应的上下文向量，同理从问句词向量 $Q$ 得到对应的上下文向量。\n",
    "\n",
    "> - 上面三层同时应用于问句和原文\n",
    "\n",
    "#### 交互层\n",
    "\n",
    "- Attention Flow Layer【注意力流层】\n",
    "\n",
    "> 将问句向量和原文向量进行耦合，并为原文中每个词生成一个问句相关的特征向量集合\n",
    "\n",
    "> 该层用于链接和融合来自原文和问句中词的信息\n",
    "\n",
    "> 不同于以往的注意力机制，将问句和原文总结概括为单一的特征向量。本文每个时刻的注意力向量都与其之前层的嵌入相关，且都可以流向之后的网络层。这种设计方案可以减缓由于过早归纳总结而导致的信息缺失。\n",
    "\n",
    "> 计算相似矩阵\n",
    "\n",
    "> - Context-to-query 注意力计算，计算query中每个word与context中word的相似度\n",
    "\n",
    "> - Query-to-context 注意力计算，取出每一行最大值\n",
    "\n",
    "> - 用context中的word进行计算\n",
    "\n",
    "- Modeling Layer【建模层】\n",
    "\n",
    "> 输入是context中单词的 query-aware representation 结果，再经过一个双层的 Bi-LSTM 捕捉给定query下，context-words之间的关系。\n",
    "\n",
    "> 使用RNN以扫描整个原文\n",
    "\n",
    "> 论文中使用的模型是：双向LSTM\n",
    "\n",
    "#### 输出层\n",
    "\n",
    "- 输出层是面向具体任务的，所以可以根据具体任务而做相应修改。如原文是输出问句对应的回答\n",
    "\n",
    "![title](img/bidaf2.png)\n",
    "\n",
    "\n",
    "### 论文地址\n",
    "\n",
    "- https://arxiv.org/pdf/1611.01603v1.pdf\n",
    "\n",
    "- http://papers.nips.cc/paper/5945-teaching-machines-to-read-and-comprehend.pdf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 附录\n",
    "\n",
    "## Attention&Transformer&BERT关系\n",
    "\n",
    "![title](img/fulu1.png)\n",
    "\n",
    "- 不过注意：BERT是一种语言模型，是用来训练词向量的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "252.467px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
