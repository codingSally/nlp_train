{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 词向量-Glove\n",
    "\n",
    "## 词向量方法总结\n",
    "\n",
    "### Bow\n",
    "\n",
    "- ont-hot 表示法\n",
    "\n",
    "- TF 表示法\n",
    "\n",
    "- TF-IDF 表示法\n",
    "\n",
    "### Word2Vec\n",
    "\n",
    "- Doc2Vec\n",
    "\n",
    "### Glove\n",
    "\n",
    "\n",
    "### Elmo\n",
    "\n",
    "\n",
    "### BERT\n",
    "\n",
    "\n",
    "## Glove\n",
    "\n",
    "### 基本概念介绍\n",
    "\n",
    "- 全称：Global Vectors for Word Representation\n",
    "\n",
    "- 2014年，斯坦福大学提出的一种新的词矩阵生成的方法\n",
    "\n",
    "- 它既利用了全局的统计信息，也利用了局部的统计信息, 什么意思呢\n",
    "\n",
    "> 利用全局统计信息指的是：它利用全局语料库构建词频矩阵\n",
    "\n",
    "> 利用局部统计信息指的是：在生成词频矩阵时，采用滑动窗口，统计词的共现\n",
    "\n",
    "- Glove是一种语言模型，可以用来生成词向量\n",
    "\n",
    "- 是一个基于全局词频统计的词表征工具，它可以把一个单词表达成一个由实数组成的向量，这些向量捕捉到了单词之间一些语义特性，比如相似性（similarity）、类比性（analogy）等\n",
    "\n",
    "- 融入全局的先验统计信息，可以加快模型的训练速度\n",
    "\n",
    "> 什么叫融入全局的先验统计信息？指的是基于语料构建的词频统计信息，可以预先存储下来，之后再训练模型的时候，直接拿来用。\n",
    "\n",
    "- 通过先验统计信息，可以控制词的相对权重\n",
    "\n",
    "> GloVe根据两个单词在上下文窗口的距离 d，提出了一个衰减函数（decreasing weighting）：decay=1/d用于计算权重，也就是说距离越远的两个单词所占总计数（total count）的权重越小。\n",
    "\n",
    "- 总之，Glove是基于统计的语言模型，用来生成词向量\n",
    "\n",
    "### Glove与LSA的区别\n",
    "\n",
    "![title](img/Glove-LSA.png)\n",
    "\n",
    "### Glove与Word2Vec的区别\n",
    "\n",
    "![title](img/glove-v2c.png)\n",
    "\n",
    "### Glove基本原理\n",
    "\n",
    "- 参见有道云笔记\n",
    "\n",
    "### 预训练模型链接\n",
    "\n",
    "- https://nlp.stanford.edu/projects/glove/\n",
    "\n",
    "### 参考博文\n",
    "\n",
    "- https://www.biaodianfu.com/glove.html\n",
    "\n",
    "- https://blog.csdn.net/coderTC/article/details/73864097\n",
    "\n",
    "- https://zhuanlan.zhihu.com/p/42073620\n",
    "\n",
    "- http://www.fanyeong.com/2018/02/19/glove-in-detail/"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction\n",
    "\n",
    "## CNN\n",
    "\n",
    "- 卷积神经网络（Convolutional Neural Network）\n",
    "\n",
    "### 主要解决的问题\n",
    "\n",
    "- 将复杂问题简化，大量参数降维成少量参数，再做处理\n",
    "\n",
    "> 因为图片像素点比较多，而且还需要考虑RGB三通道，如果全维度计算的话，需要的参数是M*N*3.参数量非常大，所以卷积希望可以降维\n",
    "\n",
    "- 保留图像特征，无论是图像做翻转、旋转、或者位置变换，都能有效的识别出来\n",
    "\n",
    "\n",
    "- 参考博文\n",
    "\n",
    "> https://easyai.tech/ai-definition/cnn/\n",
    "\n",
    "### 基本概念\n",
    "\n",
    "- 标准定义\n",
    "\n",
    "> “卷积神经网络”表示在网络中采用称为卷积的数学运算。\n",
    "\n",
    "> 卷积是一种特殊的线性操作\n",
    "\n",
    "> 卷积网络是一种特殊的神经网络，他们在至少一个层中使用卷积代替一般矩阵乘法\n",
    "\n",
    "- 组成部分\n",
    "\n",
    "> CNN由一个或多个卷积层和顶端的全连通层（对应经典的神经网络）组成，同时也包含关联权重和池化层\n",
    "\n",
    "- 实际应用\n",
    "\n",
    "> 是一种可用于处理网格结构的神经网络，如\n",
    "\n",
    "> 1. 图像处理【可以看作是二维的像素网格】\n",
    "\n",
    "> 2. 时序数据处理【可以认为是在时间轴上有规律地采样形成的一维网络】\n",
    "\n",
    "### 三个重要思想\n",
    "\n",
    "- 卷积运算通过三个重要思想来帮助改机机器学习系统：稀疏交互、参数共享和等变表示。\n",
    "\n",
    "#### 稀疏交互\n",
    "\n",
    "- 是指: 不是每个输出单元与输入单元都产生交互\n",
    "\n",
    "![title](img/CNN1.png)\n",
    "\n",
    "> 通过卷积核提取局部特征，而不是每一个像素点的特征，所以不是每个输出单元都与输入单元产生交互\n",
    "\n",
    "#### 参数共享\n",
    "\n",
    "- 是指: 多个函数相同参数\n",
    "\n",
    "![title](img/CNN2.png)\n",
    "\n",
    "> 我理解的是，比如像之前那种全连接网络来说，每一个当前输入需要与下一层的所有输出相连接，从而需要学习的参数就是M*N的关系。而使用了卷积核之后呢，同一个位置对应的多个输入点，可以共享同一份参数【可以通过网格平移想象一下】\n",
    "\n",
    "#### 平移等变\n",
    "\n",
    "- 是指: 输入改变，输出也以同样的方式改变\n",
    "\n",
    "![title](img/CNN3.png)\n",
    "\n",
    "### 基本原理\n",
    "\n",
    "#### CNN组成部分\n",
    "\n",
    "- 输入层：主要负责接收数据\n",
    "\n",
    "> 与传统机器学习一样，送模型之前需要进行预处理，常见的三种预处理方式\n",
    "\n",
    "> 1. 去均值\n",
    "\n",
    "> 2. 归一化\n",
    "\n",
    "> 3. PCA|SVD降维\n",
    "\n",
    "- 卷积层：主要负责提取图像中的局部特征【提取局部特征】\n",
    "\n",
    "> 这个过程我们可以理解为我们使用一个过滤器（卷积核）来过滤图像的各个小区域，从而得到这些小区域的特征值。\n",
    "\n",
    "> 在具体应用中，往往有多个卷积核，可以认为，每个卷积核代表了一种图像模式，如果某个图像块与此卷积核卷积出的值大，则认为此图像块十分接近于此卷积核(每个卷积核代表一种模式)。如果我们设计了6个卷积核，可以理解：我们认为这个图像上有6种底层纹理模式，也就是我们用6中基础模式就能描绘出一副图像。\n",
    "\n",
    "- 激活层：对卷积层的输出，做一次非线性映射【非线性变换】\n",
    "\n",
    "> 常见的激活函数：Sigmoid函数、Tanh函数、ReLU、Leaky ReLU、ELU、Maxout\n",
    "\n",
    "> 激活函数使用建议：首先ReLU，因为迭代速度快，但是有可能效果不佳。如果ReLU失效的情况下，考虑使用Leaky ReLU或者Maxout，此时一般情况都可以解决。Tanh函数在文本和音频处理有比较好的效果。\n",
    "\n",
    "- 池化层：用来大幅降低参数两集【降维、避免过拟合、提高模型容错性】\n",
    "\n",
    "> 池化层简单理解就是下采样，它可以降低数据维度，避免过拟合，同时可以压缩数据和参数的数量以及提高模型的容错性。\n",
    "\n",
    "> 之所以这么做的原因，是因为即使做完了卷积，图像仍然很大（因为卷积核比较小），所以为了降低数据维度，就进行下采样。\n",
    "\n",
    "> 池化层主要有最大池化层和平均池化层\n",
    "\n",
    "- 全连接层：类似传统神经网络的部分，用来输出想要的结果【输出结果】\n",
    "\n",
    "> 经过卷积层和池化层降维过的数据，全连接层才能”跑得动”，不然数据量太大，计算成本高，效率低下。\n",
    "\n",
    "> 在全连接层之前，如果神经元数目过大，学习能力强，有可能出现过拟合。因此，可以引入dropout操作，来随机删除神经网络中的部分神经元.【dropout的工作原理后面需要梳理】\n",
    "\n",
    "> 还可以进行局部归一化（LRN）、数据增强等操作，来增加鲁棒性.\n",
    "\n",
    "- Dropout【避免过拟合】\n",
    "\n",
    "> 一定概率关闭神经元，避免过拟合\n",
    "\n",
    "> 原理待补充\n",
    "\n",
    "### 优缺点\n",
    "\n",
    "#### 优点\n",
    "\n",
    "- 共享卷积核（共享参数）、优化计算量、可处理高维数据\n",
    "\n",
    "- 无需手动选择特征，只要训练好权重，就可得到特征\n",
    "\n",
    "- 深层次抽取信息丰富，表达效果好\n",
    "\n",
    "#### 缺点\n",
    "\n",
    "- 需要调参、需要大量训练样本\n",
    "\n",
    "- 训练迭代次数比较多，需要GPU资源\n",
    "\n",
    "- 难以直观解释\n",
    "\n",
    "### TextCNN\n",
    "\n",
    "#### 基本概念\n",
    "\n",
    "- TextCNN是卷积神经网络的一种\n",
    "\n",
    "- 与传统图像的CNN网络相比, textCNN 在网络结构上没有任何变化(甚至更加简单了), textCNN只有一层卷积,一层max-pooling, 最后将输出外接softmax 来n分类。\n",
    "\n",
    "- 将卷积神经网络CNN应用到文本分类任务，利用多个不同size的kernel来提取句子中的关键信息（类似于多窗口大小的ngram），从而能够更好地捕捉局部相关性。\n",
    "\n",
    "#### 网络结构\n",
    "\n",
    "![title](img/TextCNN1.png)\n",
    "\n",
    "\n",
    "![title](img/TextCNN3.png)\n",
    "\n",
    "#### 原理图\n",
    "\n",
    "![title](img/TextCNN2.png)\n",
    "\n",
    "\n",
    "- 1. CNN中卷积层中的每一个叫做卷积核，而不是卷积，卷积是一种运算\n",
    "\n",
    "- 2. 原理图中有3个不同的区域大小[卷积核的大小]，分别是 n=2、3、4， 类似于ngram中的n= 2、3、4,即和最近多少个词有关系\n",
    "\n",
    "- 3. filter的理解：\n",
    "\n",
    "> 在有的文档中，一个filter等同于一个卷积核：只是指定了卷积核的长宽深；\n",
    "\n",
    "> 而有的情况（例如tensorflow等框架中，filter参数通常指定了卷积核的长、宽、深、个数四个参数），filter包含了卷积核形状和卷积核数量的概念：即filter既指定了卷积核的长宽深，也指定了卷积核的数量。\n",
    "\n",
    "![title](img/TextCNN4.png)\n",
    "\n",
    "> 所以这里每种类型的卷积核2，3，4是为了模拟类似n元语法的思想。而每种卷积核又有不同的filter，如word2vec、Glove是为了多元化词向量，最终的目标都是为了捕捉更多的不同特征。\n",
    "\n",
    "> filter初始值是随机初始化的，参数值是通过训练不断更新的。\n",
    "\n",
    "- 4. 所以原理图中，并没有指出是6个卷积核还是3个，但是指出了卷积核的类别(3种)，所以可以按自己理解就行\n",
    "\n",
    "- 5. channel的理解：\n",
    "\n",
    "> 每个卷积层中卷积核的数量, 在这里就是6\n",
    "\n",
    "![title](img/TextCNN5.png)\n",
    "\n",
    "![title](img/TextCNN6.png)\n",
    "\n",
    "> 原文中，每个卷积核类型，采用双通道\n",
    "\n",
    "- 6. 原文中使用了L2正则化项 \n",
    "\n",
    "- 7. 所以在这个原理图中，就是3种类型的卷积核（n=2,3,4）,6个过滤器，整个卷积层有6个channels.\n",
    "\n",
    "#### 输入层设计\n",
    "\n",
    "- CNN-rand(单channel)\n",
    "> 设计好 embedding_size 这个超参数后, 对不同单词的向量作随机初始化, 后续BP的时候作调整\n",
    "\n",
    "- CNN-Static(单channel)\n",
    "> 拿 pre-trained vectors from word2vec, FastText or GloVe 直接用, 训练过程中不再调整词向量. 这也算是迁移学习的一种思想\n",
    "\n",
    "- CNN-non-static(单channel)\n",
    "> pre-trained vectors + fine tuning , 即拿word2vec训练好的词向量初始化, 训练过程中再对它们微调\n",
    "\n",
    "- CNN-multiple channel(多channels)\n",
    "> 类比于图像中的RGB通道, 这里也可以用 static 与 non-static 搭两个通道来搞.\n",
    "\n",
    "输入句子长度不一致怎么办：补0\n",
    "\n",
    "\n",
    "![title](img/TextCNN10.png)\n",
    "\n",
    "#### 超参数\n",
    "\n",
    "- sequence_length：定长处理, 超过的截断, 不足的补0\n",
    "\n",
    "- num_classes：多分类, 分为几类\n",
    "\n",
    "- vocabulary_size：语料库的词典大小\n",
    "\n",
    "- embedding_size：词向量的维度\n",
    "\n",
    "- filter_size ：一般是个array, 输入多个filter\n",
    "\n",
    "\n",
    "#### 实验的结论\n",
    "\n",
    "- 预训练word2vec 或GloVe效果好于onehot\n",
    "\n",
    "- 卷积核大小：有较大影响，可取1~10，越长文本越大。\n",
    "\n",
    "- 卷积核的数量：有较大的影响，一般取100~600 ，一般可使用Dropout（0~0.5）。\n",
    "\n",
    "- 选用ReLU 和 tanh作为激活函数\n",
    "\n",
    "- 使用1-max pooling\n",
    "\n",
    "- 当增加的feature map导致性能降低时，例如：大于0.5的Dropout。\n",
    "\n",
    "- 使用交叉验证，评估模型性能\n",
    "\n",
    "- 使用预训练模型效果比较好\n",
    "\n",
    "![title](img/TextCNN8.png)\n",
    "\n",
    "- 使用多通道没有达到预期的效果【预期想避免过拟合（通过正则化就可以实现）】\n",
    "\n",
    "![title](img/TextCNN9.png)\n",
    "\n",
    "#### 论文地址\n",
    "\n",
    "- https://arxiv.org/pdf/1408.5882.pdf\n",
    "\n",
    "- https://arxiv.org/pdf/1510.03820.pdf\n",
    "\n",
    "\n",
    "#### 参考博文\n",
    "\n",
    "> https://www.cnblogs.com/bymo/p/9675654.html\n",
    "\n",
    "> https://www.cnblogs.com/ModifyRong/p/11319301.html\n",
    "\n",
    "### 背景知识\n",
    "\n",
    "- 什么是卷积运算\n",
    "\n",
    "> 卷积是一种积分运算，用来求两个曲线重叠区域面积\n",
    "\n",
    "> 可以看做加权求和，可以用来消除噪声、特征增强\n",
    "\n",
    "> 通过卷积把一个点的像素值用它周围的点的像素值的加权平均代替\n",
    "\n",
    "> 卷积在深度学习中，可以简单的理解为“加权求和”\n",
    "\n",
    "- 计算公式\n",
    "\n",
    "> 离散变量卷积运算公式\n",
    "\n",
    "![title](img/CNN4.png)\n",
    "\n",
    "> 连续变量卷积运算公式\n",
    "\n",
    "![title](img/CNN5.png)\n",
    "\n",
    "- 计算方式\n",
    "\n",
    "> 注意和矩阵运算方式有点不同，卷积是倒着计算的【但是在神经网络中还是正着计算的（所以，实际上，在神经网络中，不是严格的卷积运算，而是一种互预算 -- 直白理解就是对应元素相乘之后加和）】\n",
    "\n",
    "- 参考博文\n",
    "\n",
    "> zhihu.com/question/22298352\n",
    "\n",
    "> https://www.zhihu.com/question/49376084\n",
    "\n",
    "> https://www.zhihu.com/question/49376084\n",
    "\n",
    "### 参考博文\n",
    "\n",
    "- https://blog.csdn.net/Daycym/article/details/90140124\n",
    "\n",
    "- https://blog.csdn.net/weixin_42813521/article/details/104991490\n",
    "\n",
    "- https://zhuanlan.zhihu.com/p/48134104"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Context-Query Interaction\n",
    "\n",
    "## 注意力机制"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "225px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
