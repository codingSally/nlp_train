{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 词向量-Glove\n",
    "\n",
    "## 词向量方法总结\n",
    "\n",
    "### Bow\n",
    "\n",
    "- ont-hot 表示法\n",
    "\n",
    "- TF 表示法\n",
    "\n",
    "- TF-IDF 表示法\n",
    "\n",
    "### Word2Vec\n",
    "\n",
    "- Doc2Vec\n",
    "\n",
    "### Glove\n",
    "\n",
    "\n",
    "### Elmo\n",
    "\n",
    "\n",
    "### BERT\n",
    "\n",
    "\n",
    "## Glove\n",
    "\n",
    "### 基本概念介绍\n",
    "\n",
    "- 全称：Global Vectors for Word Representation\n",
    "\n",
    "- 2014年，斯坦福大学提出的一种新的词矩阵生成的方法\n",
    "\n",
    "- 它既利用了全局的统计信息，也利用了局部的统计信息, 什么意思呢\n",
    "\n",
    "> 利用全局统计信息指的是：它利用全局语料库构建词频矩阵\n",
    "\n",
    "> 利用局部统计信息指的是：在生成词频矩阵时，采用滑动窗口，统计词的共现\n",
    "\n",
    "- Glove是一种语言模型，可以用来生成词向量\n",
    "\n",
    "- 是一个基于全局词频统计的词表征工具，它可以把一个单词表达成一个由实数组成的向量，这些向量捕捉到了单词之间一些语义特性，比如相似性（similarity）、类比性（analogy）等\n",
    "\n",
    "- 融入全局的先验统计信息，可以加快模型的训练速度\n",
    "\n",
    "> 什么叫融入全局的先验统计信息？指的是基于语料构建的词频统计信息，可以预先存储下来，之后再训练模型的时候，直接拿来用。\n",
    "\n",
    "- 通过先验统计信息，可以控制词的相对权重\n",
    "\n",
    "> GloVe根据两个单词在上下文窗口的距离 d，提出了一个衰减函数（decreasing weighting）：decay=1/d用于计算权重，也就是说距离越远的两个单词所占总计数（total count）的权重越小。\n",
    "\n",
    "- 总之，Glove是基于统计的语言模型，用来生成词向量\n",
    "\n",
    "### Glove与LSA的区别\n",
    "\n",
    "![title](img/Glove-LSA.png)\n",
    "\n",
    "### Glove与Word2Vec的区别\n",
    "\n",
    "![title](img/glove-v2c.png)\n",
    "\n",
    "### Glove基本原理\n",
    "\n",
    "- 参见有道云笔记\n",
    "\n",
    "### 预训练模型链接\n",
    "\n",
    "- https://nlp.stanford.edu/projects/glove/\n",
    "\n",
    "### 参考博文\n",
    "\n",
    "- https://www.biaodianfu.com/glove.html\n",
    "\n",
    "- https://blog.csdn.net/coderTC/article/details/73864097\n",
    "\n",
    "- https://zhuanlan.zhihu.com/p/42073620\n",
    "\n",
    "- http://www.fanyeong.com/2018/02/19/glove-in-detail/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction\n",
    "\n",
    "## CNN\n",
    "\n",
    "- 卷积神经网络（Convolutional Neural Network）\n",
    "\n",
    "### 主要解决的问题\n",
    "\n",
    "- 将复杂问题简化，大量参数降维成少量参数，再做处理\n",
    "\n",
    "> 因为图片像素点比较多，而且还需要考虑RGB三通道，如果全维度计算的话，需要的参数是M*N*3.参数量非常大，所以卷积希望可以降维\n",
    "\n",
    "- 保留图像特征，无论是图像做翻转、旋转、或者位置变换，都能有效的识别出来\n",
    "\n",
    "\n",
    "- 参考博文\n",
    "\n",
    "> https://easyai.tech/ai-definition/cnn/\n",
    "\n",
    "### 基本概念\n",
    "\n",
    "- 标准定义\n",
    "\n",
    "> “卷积神经网络”表示在网络中采用称为卷积的数学运算。\n",
    "\n",
    "> 卷积是一种特殊的线性操作\n",
    "\n",
    "> 卷积网络是一种特殊的神经网络，他们在至少一个层中使用卷积代替一般矩阵乘法\n",
    "\n",
    "- 组成部分\n",
    "\n",
    "> CNN由一个或多个卷积层和顶端的全连通层（对应经典的神经网络）组成，同时也包含关联权重和池化层\n",
    "\n",
    "- 实际应用\n",
    "\n",
    "> 是一种可用于处理网格结构的神经网络，如\n",
    "\n",
    "> 1. 图像处理【可以看作是二维的像素网格】\n",
    "\n",
    "> 2. 时序数据处理【可以认为是在时间轴上有规律地采样形成的一维网络】\n",
    "\n",
    "### 三个重要思想\n",
    "\n",
    "- 卷积运算通过三个重要思想来帮助改机机器学习系统：稀疏交互、参数共享和等变表示。\n",
    "\n",
    "#### 稀疏交互\n",
    "\n",
    "- 是指: 不是每个输出单元与输入单元都产生交互\n",
    "\n",
    "![title](img/CNN1.png)\n",
    "\n",
    "> 通过卷积核提取局部特征，而不是每一个像素点的特征，所以不是每个输出单元都与输入单元产生交互\n",
    "\n",
    "#### 参数共享\n",
    "\n",
    "- 是指: 多个函数相同参数\n",
    "\n",
    "![title](img/CNN2.png)\n",
    "\n",
    "> 我理解的是，比如像之前那种全连接网络来说，每一个当前输入需要与下一层的所有输出相连接，从而需要学习的参数就是M*N的关系。而使用了卷积核之后呢，同一个位置对应的多个输入点，可以共享同一份参数【可以通过网格平移想象一下】\n",
    "\n",
    "#### 平移等变\n",
    "\n",
    "- 是指: 输入改变，输出也以同样的方式改变\n",
    "\n",
    "![title](img/CNN3.png)\n",
    "\n",
    "### 基本原理\n",
    "\n",
    "#### CNN组成部分\n",
    "\n",
    "- 输入层：主要负责接收数据\n",
    "\n",
    "> 与传统机器学习一样，送模型之前需要进行预处理，常见的三种预处理方式\n",
    "\n",
    "> 1. 去均值\n",
    "\n",
    "> 2. 归一化\n",
    "\n",
    "> 3. PCA|SVD降维\n",
    "\n",
    "- 卷积层：主要负责提取图像中的局部特征【提取局部特征】\n",
    "\n",
    "> 这个过程我们可以理解为我们使用一个过滤器（卷积核）来过滤图像的各个小区域，从而得到这些小区域的特征值。\n",
    "\n",
    "> 在具体应用中，往往有多个卷积核，可以认为，每个卷积核代表了一种图像模式，如果某个图像块与此卷积核卷积出的值大，则认为此图像块十分接近于此卷积核(每个卷积核代表一种模式)。如果我们设计了6个卷积核，可以理解：我们认为这个图像上有6种底层纹理模式，也就是我们用6中基础模式就能描绘出一副图像。\n",
    "\n",
    "- 激活层：对卷积层的输出，做一次非线性映射【非线性变换】\n",
    "\n",
    "> 常见的激活函数：Sigmoid函数、Tanh函数、ReLU、Leaky ReLU、ELU、Maxout\n",
    "\n",
    "> 激活函数使用建议：首先ReLU，因为迭代速度快，但是有可能效果不佳。如果ReLU失效的情况下，考虑使用Leaky ReLU或者Maxout，此时一般情况都可以解决。Tanh函数在文本和音频处理有比较好的效果。\n",
    "\n",
    "- 池化层：用来大幅降低参数两集【降维、避免过拟合、提高模型容错性】\n",
    "\n",
    "> 池化层简单理解就是下采样，它可以降低数据维度，避免过拟合，同时可以压缩数据和参数的数量以及提高模型的容错性。\n",
    "\n",
    "> 之所以这么做的原因，是因为即使做完了卷积，图像仍然很大（因为卷积核比较小），所以为了降低数据维度，就进行下采样。\n",
    "\n",
    "> 池化层主要有最大池化层和平均池化层\n",
    "\n",
    "- 全连接层：类似传统神经网络的部分，用来输出想要的结果【输出结果】\n",
    "\n",
    "> 经过卷积层和池化层降维过的数据，全连接层才能”跑得动”，不然数据量太大，计算成本高，效率低下。\n",
    "\n",
    "> 在全连接层之前，如果神经元数目过大，学习能力强，有可能出现过拟合。因此，可以引入dropout操作，来随机删除神经网络中的部分神经元.【dropout的工作原理后面需要梳理】\n",
    "\n",
    "> 还可以进行局部归一化（LRN）、数据增强等操作，来增加鲁棒性.\n",
    "\n",
    "- Dropout【避免过拟合】\n",
    "\n",
    "> 一定概率关闭神经元，避免过拟合\n",
    "\n",
    "> 原理待补充\n",
    "\n",
    "### 优缺点\n",
    "\n",
    "#### 优点\n",
    "\n",
    "- 共享卷积核（共享参数）、优化计算量、可处理高维数据\n",
    "\n",
    "- 无需手动选择特征，只要训练好权重，就可得到特征\n",
    "\n",
    "- 深层次抽取信息丰富，表达效果好\n",
    "\n",
    "#### 缺点\n",
    "\n",
    "- 需要调参、需要大量训练样本\n",
    "\n",
    "- 训练迭代次数比较多，需要GPU资源\n",
    "\n",
    "- 难以直观解释\n",
    "\n",
    "### TextCNN\n",
    "\n",
    "#### 基本概念\n",
    "\n",
    "- TextCNN是卷积神经网络的一种\n",
    "\n",
    "- 与传统图像的CNN网络相比, textCNN 在网络结构上没有任何变化(甚至更加简单了), textCNN只有一层卷积,一层max-pooling, 最后将输出外接softmax 来n分类。\n",
    "\n",
    "#### 网络结构\n",
    "\n",
    "\n",
    "#### 网络模型\n",
    "\n",
    "\n",
    "#### 参考博文\n",
    "\n",
    "> https://www.cnblogs.com/bymo/p/9675654.html\n",
    "\n",
    "> https://www.cnblogs.com/ModifyRong/p/11319301.html\n",
    "\n",
    "### 背景知识\n",
    "\n",
    "- 什么是卷积运算\n",
    "\n",
    "> 卷积是一种积分运算，用来求两个曲线重叠区域面积\n",
    "\n",
    "> 可以看做加权求和，可以用来消除噪声、特征增强\n",
    "\n",
    "> 通过卷积把一个点的像素值用它周围的点的像素值的加权平均代替\n",
    "\n",
    "> 卷积在深度学习中，可以简单的理解为“加权求和”\n",
    "\n",
    "- 计算公式\n",
    "\n",
    "> 离散变量卷积运算公式\n",
    "\n",
    "![title](img/CNN4.png)\n",
    "\n",
    "> 连续变量卷积运算公式\n",
    "\n",
    "![title](img/CNN5.png)\n",
    "\n",
    "- 计算方式\n",
    "\n",
    "> 注意和矩阵运算方式有点不同，卷积是倒着计算的【但是在神经网络中还是正着计算的（所以，实际上，在神经网络中，不是严格的卷积运算，而是一种互预算 -- 直白理解就是对应元素相乘之后加和）】\n",
    "\n",
    "- 参考博文\n",
    "\n",
    "> zhihu.com/question/22298352\n",
    "\n",
    "> https://www.zhihu.com/question/49376084\n",
    "\n",
    "> https://www.zhihu.com/question/49376084\n",
    "\n",
    "### 参考博文\n",
    "\n",
    "- https://blog.csdn.net/Daycym/article/details/90140124\n",
    "\n",
    "- https://blog.csdn.net/weixin_42813521/article/details/104991490\n",
    "\n",
    "- https://zhuanlan.zhihu.com/p/48134104"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- ## RNN\n",
    "\n",
    "## Transformer -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Context-Query Interaction\n",
    "\n",
    "## 注意力机制"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
