{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 词向量-Glove\n",
    "\n",
    "## 词向量方法总结\n",
    "\n",
    "### Bow\n",
    "\n",
    "- ont-hot 表示法\n",
    "\n",
    "- TF 表示法\n",
    "\n",
    "- TF-IDF 表示法\n",
    "\n",
    "### Word2Vec\n",
    "\n",
    "- Doc2Vec\n",
    "\n",
    "### Glove\n",
    "\n",
    "\n",
    "### Elmo\n",
    "\n",
    "\n",
    "### BERT\n",
    "\n",
    "\n",
    "## Glove\n",
    "\n",
    "### 基本概念介绍\n",
    "\n",
    "- 全称：Global Vectors for Word Representation\n",
    "\n",
    "- 2014年，斯坦福大学提出的一种新的词矩阵生成的方法\n",
    "\n",
    "- 它既利用了全局的统计信息，也利用了局部的统计信息, 什么意思呢\n",
    "\n",
    "> 利用全局统计信息指的是：它利用全局语料库构建词频矩阵\n",
    "\n",
    "> 利用局部统计信息指的是：在生成词频矩阵时，采用滑动窗口，统计词的共现\n",
    "\n",
    "- Glove是一种语言模型，可以用来生成词向量\n",
    "\n",
    "- 是一个基于全局词频统计的词表征工具，它可以把一个单词表达成一个由实数组成的向量，这些向量捕捉到了单词之间一些语义特性，比如相似性（similarity）、类比性（analogy）等\n",
    "\n",
    "- 融入全局的先验统计信息，可以加快模型的训练速度\n",
    "\n",
    "> 什么叫融入全局的先验统计信息？指的是基于语料构建的词频统计信息，可以预先存储下来，之后再训练模型的时候，直接拿来用。\n",
    "\n",
    "- 通过先验统计信息，可以控制词的相对权重\n",
    "\n",
    "> GloVe根据两个单词在上下文窗口的距离 d，提出了一个衰减函数（decreasing weighting）：decay=1/d用于计算权重，也就是说距离越远的两个单词所占总计数（total count）的权重越小。\n",
    "\n",
    "- 总之，Glove是基于统计的语言模型，用来生成词向量\n",
    "\n",
    "### Glove与LSA的区别\n",
    "\n",
    "![title](img/Glove-LSA.png)\n",
    "\n",
    "### Glove与Word2Vec的区别\n",
    "\n",
    "![title](img/glove-v2c.png)\n",
    "\n",
    "### Glove基本原理\n",
    "\n",
    "- 参见有道云笔记\n",
    "\n",
    "### 预训练模型链接\n",
    "\n",
    "- https://nlp.stanford.edu/projects/glove/\n",
    "\n",
    "### 参考博文\n",
    "\n",
    "- https://www.biaodianfu.com/glove.html\n",
    "\n",
    "- https://blog.csdn.net/coderTC/article/details/73864097\n",
    "\n",
    "- https://zhuanlan.zhihu.com/p/42073620\n",
    "\n",
    "- http://www.fanyeong.com/2018/02/19/glove-in-detail/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction\n",
    "\n",
    "## CNN\n",
    "\n",
    "- 卷积神经网络（Convolutional Neural Network）\n",
    "\n",
    "### 基本概念\n",
    "\n",
    "- 标准定义\n",
    "\n",
    "> “卷积神经网络”表示在网络中采用称为卷积的数学运算。\n",
    "\n",
    "> 卷积是一种特殊的线性操作\n",
    "\n",
    "> 卷积网络是一种特殊的神经网络，他们在至少一个层中使用卷积代替一般矩阵乘法\n",
    "\n",
    "- 组成部分\n",
    "\n",
    "> CNN由一个或多个卷积层和顶端的全连通层（对应经典的神经网络）组成，同时也包含关联权重和池化层\n",
    "\n",
    "- 实际应用\n",
    "\n",
    "> 是一种可用于处理网格结构的神经网络，如\n",
    "\n",
    "> 1. 图像处理【可以看作是二维的像素网格】\n",
    "\n",
    "> 2. 时序数据处理【可以认为是在时间轴上有规律地采样形成的一维网络】\n",
    "\n",
    "### 三个重要思想\n",
    "\n",
    "- 卷积运算通过三个重要思想来帮助改机机器学习系统：稀疏交互、参数共享和等变表示。\n",
    "\n",
    "#### 稀疏交互\n",
    "\n",
    "- 是指: 不是每个输出单元与输入单元都产生交互\n",
    "\n",
    "![title](img/CNN1.png)\n",
    "\n",
    "> 通过卷积核提取局部特征，而不是每一个像素点的特征，所以不是每个输出单元都与输入单元产生交互\n",
    "\n",
    "#### 参数共享\n",
    "\n",
    "- 是指: 多个函数相同参数\n",
    "\n",
    "![title](img/CNN2.png)\n",
    "\n",
    "> 我理解的是，比如像之前那种全连接网络来说，每一个当前输入需要与下一层的所有输出相连接，从而需要学习的参数就是M*N的关系。而使用了卷积核之后呢，同一个位置对应的多个输入点，可以共享同一份参数【可以通过网格平移想象一下】\n",
    "\n",
    "#### 平移等变\n",
    "\n",
    "- 是指: 输入改变，输出也以同样的方式改变\n",
    "\n",
    "![title](img/CNN3.png)\n",
    "\n",
    "### 基本原理\n",
    "\n",
    "\n",
    "### 优缺点\n",
    "\n",
    "\n",
    "### TextCNN\n",
    "\n",
    "\n",
    "### 背景知识\n",
    "\n",
    "- 什么是卷积运算\n",
    "\n",
    "> 卷积是一种积分运算，用来求两个曲线重叠区域面积\n",
    "\n",
    "> 可以看做加权求和，可以用来消除噪声、特征增强\n",
    "\n",
    "> 通过卷积把一个点的像素值用它周围的点的像素值的加权平均代替\n",
    "\n",
    "> 卷积在深度学习中，可以简单的理解为“加权求和”\n",
    "\n",
    "- 计算公式\n",
    "\n",
    "> 离散变量卷积运算公式\n",
    "\n",
    "![title](img/CNN4.png)\n",
    "\n",
    "> 连续变量卷积运算公式\n",
    "\n",
    "![title](img/CNN5.png)\n",
    "\n",
    "- 计算方式\n",
    "\n",
    "> 注意和矩阵运算方式有点不同，卷积是倒着计算的【但是在神经网络中还是正着计算的（所以，实际上，在神经网络中，不是严格的卷积运算，而是一种互预算 -- 直白理解就是对应元素相乘之后加和）】\n",
    "\n",
    "- 参考博文\n",
    "\n",
    "> zhihu.com/question/22298352\n",
    "\n",
    "> https://www.zhihu.com/question/49376084\n",
    "\n",
    "> https://www.zhihu.com/question/49376084\n",
    "\n",
    "### 参考博文\n",
    "\n",
    "- https://blog.csdn.net/Daycym/article/details/90140124\n",
    "\n",
    "- https://blog.csdn.net/weixin_42813521/article/details/104991490\n",
    "\n",
    "- https://zhuanlan.zhihu.com/p/48134104\n",
    "\n",
    "## RNN\n",
    "\n",
    "## Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Context-Query Interaction\n",
    "\n",
    "## 注意力机制"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
