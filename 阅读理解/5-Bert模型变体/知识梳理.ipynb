{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ERNIE\n",
    "\n",
    "- Enhanced Representation through Knowledge Integration\n",
    "\n",
    "- 百度开发\n",
    "\n",
    "## 提出背景\n",
    "\n",
    "- 前面总结的BERT 模型，通过随机屏蔽15%的字或者word，利用 Transformer 的多层 self-attention 双向建模能力，在各项nlp 下游任务中(如 sentence pair classification task, singe sentence classification task, question answering task) 都取得了SOTA的成绩。\n",
    "\n",
    "- 但是，BERT 模型主要是聚焦在针对字或者英文word粒度的完形填空学习上面，没有充分利用训练数据当中词法结构，语法结构，以及语义信息去学习建模\n",
    "\n",
    "- BERT没有基于语法结构和句法结构进行建模，那么对于一个新出现的词，很难给出一个很好的向量表示\n",
    "\n",
    "- 而ERNIE 通过对训练数据中的词法结构，语法结构，语义信息进行统一建模，极大地增强了通用语义表示能力，在多项任务中均取得了大幅度超越BERT的效果\n",
    "\n",
    "\n",
    "## 基本概念\n",
    "\n",
    "- 主要贡献\n",
    "\n",
    "> 通过实体和短语mask能够学习语法和句法信息的语言模型\n",
    "\n",
    "> 在很多中文自然语言处理任务上达到state-of-the art\n",
    "\n",
    "> 放出了代码和预训练模型\n",
    "\n",
    "- 基本方法\n",
    "\n",
    "> 与Bert类似，模型结构没有变，主要区别在于ERNIE不像BERT只采用了单个字级别的MASK，而是多样化了不同级别的MASK -- 单字、实体、短语\n",
    "\n",
    "- 训练数据集\n",
    "\n",
    "> 中文维基百科，百度百科，百度新闻，百度贴吧\n",
    "\n",
    "- 其他细节\n",
    "\n",
    "> 中文繁体转为简体\n",
    "\n",
    "> 英文大写转为小写\n",
    "\n",
    "> 词表大小17964\n",
    "\n",
    "\n",
    "## 基本架构\n",
    "\n",
    "- ERNIE 初探\n",
    "\n",
    "![title](img/ernie1.png)\n",
    "\n",
    "\n",
    "> 由上结构可以对比到ERNIE与BERT在MASK策略上面的不同\n",
    "\n",
    "- ERNIE 的encoder \n",
    "\n",
    "![title](img/ernie2.png)\n",
    "\n",
    "\n",
    "- 相比transformer , ERNIE 基本上是 transformer 的encoder 部分，并且encoder 在结构上是全部一样的，但是并不共享权重， 具体区别如下：\n",
    "\n",
    "> Transformer: 6 encoder layers, 512 hidden units, 8 attention heads\n",
    "\n",
    "> ERNIE Base: 12 encoder layers, 768 hidden units, 12 attention heads\n",
    "\n",
    "> ERNIE Large: 24 encoder layers,1024 hidden units, 16 attention heads\n",
    "\n",
    "- 从输入上来看第一个输入是一个特殊的CLS, CLS 表示分类任务就像 transformer 的一般的encoder, ERINE 将一序列的words 输入到encoder 中\n",
    "\n",
    "- 每层使用self-attention, feed-word network, 然后把结果传入到下一个encoder\n",
    "\n",
    "\n",
    "## 基本原理\n",
    "\n",
    "- ERNIE 的encoder 原理细节\n",
    "\n",
    "![title](img/ernie3.png)\n",
    "\n",
    "> encoder 由两层构成, 首先流入self-attention layer，self-attention layer 输出流入 feed-forward 神经网络\n",
    "\n",
    "> 最下层的encoder的输入是embedding的向量, 其他的encoder的输入，便是更下层的encoder的输出, 一般设置输入的vectors 的维度为512\n",
    "\n",
    "### ERNIE1.0\n",
    "\n",
    "- ERNIE 1.0 改进的两种 masking 策略\n",
    "\n",
    "> 一种是基于phrase (在这里是短语 比如 a series of, written等)的masking策略\n",
    "\n",
    "> 另外一种是基于 entity(在这里是人名，位置, 组织，产品等名词 比如Apple, J.K. Rowling)的masking 策略\n",
    "\n",
    "> 在ERNIE 当中，将由多个字组成的phrase 或者entity 当成一个统一单元，相比于bert 基于字的mask, 这个单元当中的的所有字在训练的时候，统一被mask\n",
    "\n",
    "\n",
    "\n",
    "### ERNIE2.0\n",
    "\n",
    "\n",
    "\n",
    "## 论文地址\n",
    "\n",
    "\n",
    "## 相关博文\n",
    "\n",
    "- https://cloud.tencent.com/developer/article/1525419"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer-XL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XLNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
