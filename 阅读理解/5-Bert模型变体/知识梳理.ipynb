{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ERNIE\n",
    "\n",
    "- Enhanced Representation through Knowledge Integration\n",
    "\n",
    "- 百度开发\n",
    "\n",
    "## 提出背景\n",
    "\n",
    "- 前面总结的BERT 模型，通过随机屏蔽15%的字或者word，利用 Transformer 的多层 self-attention 双向建模能力，在各项nlp 下游任务中(如 sentence pair classification task, singe sentence classification task, question answering task) 都取得了SOTA的成绩。\n",
    "\n",
    "- 但是，BERT 模型主要是聚焦在针对字或者英文word粒度的完形填空学习上面，没有充分利用训练数据当中词法结构，语法结构，以及语义信息去学习建模\n",
    "\n",
    "- BERT没有基于语法结构和句法结构进行建模，那么对于一个新出现的词，很难给出一个很好的向量表示\n",
    "\n",
    "- 而ERNIE 通过对训练数据中的词法结构，语法结构，语义信息进行统一建模，极大地增强了通用语义表示能力，在多项任务中均取得了大幅度超越BERT的效果\n",
    "\n",
    "\n",
    "## 基本概念\n",
    "\n",
    "- 主要贡献\n",
    "\n",
    "> 通过实体和短语mask能够学习语法和句法信息的语言模型\n",
    "\n",
    "> 在很多中文自然语言处理任务上达到state-of-the art\n",
    "\n",
    "> 放出了代码和预训练模型\n",
    "\n",
    "- 基本方法\n",
    "\n",
    "> 与Bert类似，模型结构没有变，主要区别在于ERNIE不像BERT只采用了单个字级别的MASK，而是多样化了不同级别的MASK -- 单字、实体、短语\n",
    "\n",
    "- 训练数据集\n",
    "\n",
    "> 中文维基百科，百度百科，百度新闻，百度贴吧\n",
    "\n",
    "- 其他细节\n",
    "\n",
    "> 中文繁体转为简体\n",
    "\n",
    "> 英文大写转为小写\n",
    "\n",
    "> 词表大小17964\n",
    "\n",
    "\n",
    "## 基本架构\n",
    "\n",
    "- ERNIE 初探\n",
    "\n",
    "![title](img/ernie1.png)\n",
    "\n",
    "\n",
    "> 由上结构可以对比到ERNIE与BERT在MASK策略上面的不同\n",
    "\n",
    "- ERNIE 的encoder \n",
    "\n",
    "![title](img/ernie2.png)\n",
    "\n",
    "\n",
    "- 相比transformer , ERNIE 基本上是 transformer 的encoder 部分，并且encoder 在结构上是全部一样的，但是并不共享权重， 具体区别如下：\n",
    "\n",
    "> Transformer: 6 encoder layers, 512 hidden units, 8 attention heads\n",
    "\n",
    "> ERNIE Base: 12 encoder layers, 768 hidden units, 12 attention heads\n",
    "\n",
    "> ERNIE Large: 24 encoder layers,1024 hidden units, 16 attention heads\n",
    "\n",
    "- 从输入上来看第一个输入是一个特殊的CLS, CLS 表示分类任务就像 transformer 的一般的encoder, ERINE 将一序列的words 输入到encoder 中\n",
    "\n",
    "- 每层使用self-attention, feed-word network, 然后把结果传入到下一个encoder\n",
    "\n",
    "\n",
    "## 基本原理\n",
    "\n",
    "- ERNIE 的encoder 原理细节\n",
    "\n",
    "![title](img/ernie3.png)\n",
    "\n",
    "> encoder 由两层构成, 首先流入self-attention layer，self-attention layer 输出流入 feed-forward 神经网络\n",
    "\n",
    "> 最下层的encoder的输入是embedding的向量, 其他的encoder的输入，便是更下层的encoder的输出, 一般设置输入的vectors 的维度为512\n",
    "\n",
    "### ERNIE1.0\n",
    "\n",
    "- ERNIE 1.0 改进的两种 masking 策略\n",
    "\n",
    "> 一种是基于phrase (在这里是短语 比如 a series of, written等)的masking策略\n",
    "\n",
    "> 另外一种是基于 entity(在这里是人名，位置, 组织，产品等名词 比如Apple, J.K. Rowling)的masking 策略\n",
    "\n",
    "> 在ERNIE 当中，将由多个字组成的phrase 或者entity 当成一个统一单元，相比于bert 基于字的mask, 这个单元当中的的所有字在训练的时候，统一被mask.\n",
    "\n",
    "> 对比直接将知识类的query 映射成向量然后直接加起来，ERNIE 通过统一mask的方式可以潜在的学习到知识的依赖以及更长的语义依赖来让模型更具泛化性\n",
    "\n",
    "![title](img/ernie4.png)\n",
    "\n",
    "- ERNIE1.0 相比于BERT的特点\n",
    "\n",
    "> 更多的语料\n",
    "\n",
    "> 实体级别连续的Mask\n",
    "\n",
    "### ERNIE2.0\n",
    "\n",
    "ERNIE 2.0: A Continual Pre-Training Framework for Language Understanding （Baidu）\n",
    "\n",
    "\n",
    "- 模型贡献\n",
    "\n",
    "> 多任务持续学习预训练框架 ERNIE 2.0\n",
    "\n",
    "> 构建三种类型的无监督任务，训练模型并刷新GLUE\n",
    "\n",
    "> 更多的数据：Reddit  搜索数据等\n",
    "\n",
    "> 参数与BERT一致\n",
    "\n",
    "- 模型架构\n",
    "\n",
    "![title](img/ernie5.png)\n",
    "\n",
    "- 连续学习\n",
    "\n",
    "> ERNIE 2.0 中有一个很重要的概念便是连续学习(Continual Learning)，连续学习的目的是在一个模型中顺序训练多个不同的任务以便在学习下个任务当中可以记住前一个学习任务学习到的结果.\n",
    "\n",
    "> 通过使用连续学习，可以不断积累新的知识，模型在新任务当中可以用历史任务学习到参数进行初始化，一般来说比直接开始新任务的学习会获得更好的效果。\n",
    "\n",
    "- 连续多任务学习特点\n",
    "\n",
    "> 不遗忘之前的训练结果\n",
    "\n",
    "> 多任务高校的进行训练\n",
    "\n",
    "> 使用杀那个以任务\n",
    "\n",
    "\n",
    "\n",
    "## 相关地址\n",
    "\n",
    "- https://aistudio.baidu.com/aistudio/competition/detail/28?isFromCcf=true\n",
    "\n",
    "- https://aistudio.baidu.com/aistudio/projectdetail/919362\n",
    "\n",
    "## 相关博文\n",
    "\n",
    "- https://cloud.tencent.com/developer/article/1525419"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer-XL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XLNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
