{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ERNIE\n",
    "\n",
    "- Enhanced Representation through Knowledge Integration\n",
    "\n",
    "- 百度开发\n",
    "\n",
    "## 提出背景\n",
    "\n",
    "- 前面总结的BERT 模型，通过随机屏蔽15%的字或者word，利用 Transformer 的多层 self-attention 双向建模能力，在各项nlp 下游任务中(如 sentence pair classification task, singe sentence classification task, question answering task) 都取得了SOTA的成绩。\n",
    "\n",
    "- 但是，BERT 模型主要是聚焦在针对字或者英文word粒度的完形填空学习上面，没有充分利用训练数据当中词法结构，语法结构，以及语义信息去学习建模\n",
    "\n",
    "- BERT没有基于语法结构和句法结构进行建模，那么对于一个新出现的词，很难给出一个很好的向量表示\n",
    "\n",
    "- 而ERNIE 通过对训练数据中的词法结构，语法结构，语义信息进行统一建模，极大地增强了通用语义表示能力，在多项任务中均取得了大幅度超越BERT的效果\n",
    "\n",
    "\n",
    "## 基本概念\n",
    "\n",
    "- 主要贡献\n",
    "\n",
    "> 通过实体和短语mask能够学习语法和句法信息的语言模型\n",
    "\n",
    "> 在很多中文自然语言处理任务上达到state-of-the art\n",
    "\n",
    "> 放出了代码和预训练模型\n",
    "\n",
    "- 基本方法\n",
    "\n",
    "> 与Bert类似，模型结构没有变，主要区别在于ERNIE不像BERT只采用了单个字级别的MASK，而是多样化了不同级别的MASK -- 单字、实体、短语\n",
    "\n",
    "- 训练数据集\n",
    "\n",
    "> 中文维基百科，百度百科，百度新闻，百度贴吧\n",
    "\n",
    "- 其他细节\n",
    "\n",
    "> 中文繁体转为简体\n",
    "\n",
    "> 英文大写转为小写\n",
    "\n",
    "> 词表大小17964\n",
    "\n",
    "\n",
    "## 基本架构\n",
    "\n",
    "- ERNIE 初探\n",
    "\n",
    "![title](img/ernie1.png)\n",
    "\n",
    "\n",
    "> 由上结构可以对比到ERNIE与BERT在MASK策略上面的不同\n",
    "\n",
    "- ERNIE 的encoder \n",
    "\n",
    "![title](img/ernie2.png)\n",
    "\n",
    "\n",
    "- 相比transformer , ERNIE 基本上是 transformer 的encoder 部分，并且encoder 在结构上是全部一样的，但是并不共享权重， 具体区别如下：\n",
    "\n",
    "> Transformer: 6 encoder layers, 512 hidden units, 8 attention heads\n",
    "\n",
    "> ERNIE Base: 12 encoder layers, 768 hidden units, 12 attention heads\n",
    "\n",
    "> ERNIE Large: 24 encoder layers,1024 hidden units, 16 attention heads\n",
    "\n",
    "- 从输入上来看第一个输入是一个特殊的CLS, CLS 表示分类任务就像 transformer 的一般的encoder, ERINE 将一序列的words 输入到encoder 中\n",
    "\n",
    "- 每层使用self-attention, feed-word network, 然后把结果传入到下一个encoder\n",
    "\n",
    "\n",
    "## 基本原理\n",
    "\n",
    "- ERNIE 的encoder 原理细节\n",
    "\n",
    "![title](img/ernie3.png)\n",
    "\n",
    "> encoder 由两层构成, 首先流入self-attention layer，self-attention layer 输出流入 feed-forward 神经网络\n",
    "\n",
    "> 最下层的encoder的输入是embedding的向量, 其他的encoder的输入，便是更下层的encoder的输出, 一般设置输入的vectors 的维度为512\n",
    "\n",
    "### ERNIE1.0\n",
    "\n",
    "- ERNIE 1.0 改进的两种 masking 策略\n",
    "\n",
    "> 一种是基于phrase (在这里是短语 比如 a series of, written等)的masking策略\n",
    "\n",
    "> 另外一种是基于 entity(在这里是人名，位置, 组织，产品等名词 比如Apple, J.K. Rowling)的masking 策略\n",
    "\n",
    "> 在ERNIE 当中，将由多个字组成的phrase 或者entity 当成一个统一单元，相比于bert 基于字的mask, 这个单元当中的的所有字在训练的时候，统一被mask.\n",
    "\n",
    "> 对比直接将知识类的query 映射成向量然后直接加起来，ERNIE 通过统一mask的方式可以潜在的学习到知识的依赖以及更长的语义依赖来让模型更具泛化性\n",
    "\n",
    "![title](img/ernie4.png)\n",
    "\n",
    "- ERNIE1.0 相比于BERT的特点\n",
    "\n",
    "> 更多的语料\n",
    "\n",
    "> 实体级别连续的Mask\n",
    "\n",
    "### ERNIE2.0\n",
    "\n",
    "ERNIE 2.0: A Continual Pre-Training Framework for Language Understanding （Baidu）\n",
    "\n",
    "\n",
    "- 模型贡献\n",
    "\n",
    "> 多任务持续学习预训练框架 ERNIE 2.0\n",
    "\n",
    "> 构建三种类型的无监督任务，训练模型并刷新GLUE\n",
    "\n",
    "> 更多的数据：Reddit  搜索数据等\n",
    "\n",
    "> 参数与BERT一致\n",
    "\n",
    "- 模型架构\n",
    "\n",
    "![title](img/ernie5.png)\n",
    "\n",
    "- 连续学习\n",
    "\n",
    "> ERNIE 2.0 中有一个很重要的概念便是连续学习(Continual Learning)，连续学习的目的是在一个模型中顺序训练多个不同的任务以便在学习下个任务当中可以记住前一个学习任务学习到的结果.\n",
    "\n",
    "> 通过使用连续学习，可以不断积累新的知识，模型在新任务当中可以用历史任务学习到参数进行初始化，一般来说比直接开始新任务的学习会获得更好的效果。\n",
    "\n",
    "- 连续多任务学习特点\n",
    "\n",
    "> 不遗忘之前的训练结果\n",
    "\n",
    "> 多任务高效的进行训练\n",
    "\n",
    "> 使用上一个任务的参数，并且新旧任务一起训练\n",
    "\n",
    "> 将每个任务分成多次迭代，框架完成不同迭代的训练自动分配\n",
    "\n",
    "- 多任务训练\n",
    "\n",
    "> Sentence level loss & word level loss\n",
    "\n",
    "> 每个任务有独立的loss function， sentence task 可以和word task 一起训练\n",
    "\n",
    "![title](img/ernie6.png)\n",
    "\n",
    "> ERNIE 2.0 用了我们前文提到的transformer 结构encoder, 结构基本一致，但是权重并不共享。\n",
    "\n",
    "- Task embedding\n",
    "\n",
    "> ERNIE 2.0 用了不同的task id 来标示预训练任务\n",
    "\n",
    "> 对应的token segment position 以及task embedding 被用来作为模型的输入\n",
    "\n",
    "> 模型结构\n",
    "\n",
    "![title](img/ernie7.png)\n",
    "\n",
    "\n",
    "- 不同的预训练任务\n",
    "\n",
    "> 词法级别的预训练任务 -- 用于获取训练数据中的词法信息\n",
    "\n",
    "> - Knowledge Masking Task : 学习当前和全局依赖\n",
    "\n",
    "> 即 ERNIE 1.0 中的entity mask 以及 phrase entity mask 来获取phrase 以及entity的先验知识，相较于 sub-word masking, 该策略可以更好的捕捉输入样本局部和全局的语义信息\n",
    "\n",
    "> - Capitalization Prediction Task : 大写用于专名识别等，小写也可用在其他任务\n",
    "\n",
    "> 大写的词比如Apple 相比于其他词通常在句子当中有特定的含义，所以在Erine 2.0 加入一个任务来判断一个词是否大写\n",
    "\n",
    "> - Token-Document Relation Prediction Task ： token存在段落A中是否token会在文档的段落B中出现\n",
    "\n",
    "> 类似于tf-idf, 预测一个词在文中的A 段落出现，是否会在文中的B 段落出现。如果一个词在文章当中的许多部分出现一般就说明这个词经常被用到或者和这个文章的主题相关。通过识别这个文中关键的的词, 这个任务可以增强模型去获取文章的关键词语的能力。\n",
    "\n",
    "\n",
    "> 语法级别的预训练任务 -- 用于获取训练数据中的语法信息\n",
    "\n",
    "> - Sentence Reordering Task : 档中的句子打乱（分成1到m段，shuffle），识别正确顺序\n",
    "\n",
    "> 在训练当中，将paragraph 随机分成1 到m 段，将所有的组合随机shuffle. 我们让pre-trained 的模型来识别所有的这些segments正确的顺序. 这便是一个k 分类任务。通常来说，这些sentence 重排序任务能够让pre-trained 模型学习到document 中不同sentence 的关系。\n",
    "\n",
    "> - Sentence Distance Task : 句子间的距离，3分类任务:0 相连的句子、 1同一文档中不相连的句子、2两篇文档间的句子 \n",
    "\n",
    "> 构建一个三分类任务来判别句子的距离，0表示两个句子是同一个文章中相邻的句子，1表示两个句子是在同一个文章，但是不相邻，2表示两个句子是不同的文章。通过构建这样一个三分类任务去判断句对 (sentence pairs) 位置关系 (包含邻近句子、文档内非邻近句子、非同文档内句子 3 种类别)，更好的建模语义相关性。\n",
    "\n",
    "> 语义级别的预训练任务 -- 用于获取训练数据中的语义信息\n",
    "\n",
    "> - Discourse Relation Task : 计算两句间的语义与修辞关系\n",
    "\n",
    "> 除了上面的distance task, ERNIE通过判断句对 (sentence pairs) 间的修辞关系 (semantic & rhetorical relation)，更好的学习句间语义。\n",
    "\n",
    "> - IR Relevance Task : 短文本信息检索关系（百度的核心）\n",
    "\n",
    "> Query-title  （搜索数据） : 0：搜索并点击 、 1：搜索并展现、 2：无关，随机替换\n",
    "\n",
    "> 这里主要是利用baidu 的日志来获取这个关系，将query 作为第一个sentence, title 作为第二个second sentence. 0 表示强关系, 1 表示弱关系，2表示无关系，通过类似google-distance 的关系来衡量 两个query之间的语义相关性，更好的建模句对相关性。\n",
    "\n",
    "- 目前来看最好的中英文预训练语言模型之一\n",
    "\n",
    "## 相关地址\n",
    "\n",
    "- https://aistudio.baidu.com/aistudio/competition/detail/28?isFromCcf=true\n",
    "\n",
    "- https://aistudio.baidu.com/aistudio/projectdetail/919362\n",
    "\n",
    "- https://gluebenchmark.com/leaderboard\n",
    "\n",
    "\n",
    "## 相关博文\n",
    "\n",
    "- https://cloud.tencent.com/developer/article/1525419"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer-XL\n",
    "\n",
    "Attentive Language Models Beyond a Fixed-Length Context（Google）\n",
    "\n",
    "## 主要目的\n",
    "\n",
    "- 解决长文本的建模，捕获超长距离依赖\n",
    "\n",
    "## 主要贡献\n",
    "\n",
    "- 循环的transformer建模机制\n",
    "\n",
    "- 一种相对位置编码方式\n",
    "\n",
    "## 前生：Vanilla Transformer \n",
    "\n",
    "- Vanilla Transformer是Transformer和Transformer-XL中间过度的一个算法\n",
    "\n",
    "### 模型原理\n",
    "\n",
    "![title](img/trans1.png)\n",
    "\n",
    "- 模型会将长文本输入, 切分Segment\n",
    "\n",
    "### 模型特点\n",
    "\n",
    "- 上下文长度受限\n",
    "\n",
    "> 字符之间的最大依赖距离受输入长度的限制，模型看不到出现在几个句子之前的单词。\n",
    "\n",
    "- 上下文碎片\n",
    "\n",
    "> 对于长度超过512个字符的文本，都是从头开始单独训练的。段与段之间没有上下文依赖性，会让训练效率低下，也会影响模型的性能。\n",
    "\n",
    "- 推理速度慢\n",
    "\n",
    "> 在测试阶段，每次预测下一个单词，都需要重新构建一遍上下文，并从头开始计算，这样的计算速度非常慢。\n",
    "\n",
    "- 最长依赖取决于segment长度\n",
    "\n",
    "- 不同segment之间不传递信息\n",
    "\n",
    "- 固定大小切分造成语义碎片\n",
    "\n",
    "\n",
    "## Transformer-XL\n",
    "\n",
    "- Transformer-XL架构在vanilla Transformer的基础上引入了两点创新：\n",
    "\n",
    "> 循环机制（Recurrence Mechanism）\n",
    "\n",
    "> 相对位置编码（Relative Positional Encoding）\n",
    "\n",
    "- 以克服Vanilla Transformer的缺点。\n",
    "\n",
    "- 与Vanilla Transformer相比，Transformer-XL的另一个优势是它可以被用于单词级和字符级的语言建模\n",
    "\n",
    "### 循环机制\n",
    "\n",
    "- 基本原理\n",
    "\n",
    "![title](img/trans2.png)\n",
    "\n",
    "\n",
    "![title](img/trans3.png)\n",
    "\n",
    "- 信息传递方式\n",
    "\n",
    "![title](img/trans4.png)\n",
    "\n",
    "\n",
    "- Transformer XL除了可以捕捉更长的文本信息外，也提高了模型预测的速度，因为我们知道Transformer XL在预测时每次是平移一个子句的长度，而不是像Vanilla Transformer一样每次只平移一个时间步。\n",
    "\n",
    "### 相对位置编码\n",
    "\n",
    "- 提出背景\n",
    "\n",
    "![title](img/trans5.png)\n",
    "\n",
    "> 描述Token之间的相对关系\n",
    "\n",
    "- 相对位置编码实现\n",
    "\n",
    "![title](img/trans6.png)\n",
    "\n",
    "![title](img/trans7.png)\n",
    "\n",
    "- 最终形态\n",
    "\n",
    "![title](img/trans8.png)\n",
    "\n",
    "\n",
    "## 总结\n",
    "\n",
    "### 优点\n",
    "\n",
    "- 在几种不同的数据集（大/小，字符级别/单词级别等）均实现了最先进的语言建模结果\n",
    "\n",
    "- 结合了深度学习的两个重要概念——循环机制和注意力机制，解决长距离依赖的问题\n",
    "\n",
    "- 学习超长的依赖关系\n",
    "\n",
    "- inference阶段快\n",
    "\n",
    "### 缺点\n",
    "\n",
    "- 未在具体的NLP任务如情感分析、QA等上应用\n",
    "\n",
    "- 没有给出与其他的基于Transformer的模型，如BERT等，对比有何优势\n",
    "\n",
    "- 训练模型需要用到大量的TPU资源\n",
    "\n",
    "\n",
    "## 论文地址\n",
    "\n",
    "- https://arxiv.org/pdf/1901.02860.pdf\n",
    "\n",
    "## 相关博文\n",
    "\n",
    "- https://blog.csdn.net/linchuhai/article/details/99995720\n",
    "\n",
    "- https://www.cnblogs.com/huangyc/p/11445150.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XLNet\n",
    "\n",
    "- XLNet是一个语言模型。和ELMO，GPT，BERT一脉相承，同时借鉴了Transformer-XL，故称XLNet（XL含义源于衣服尺码，意思是模型横向更宽）\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## 相关博文\n",
    "\n",
    "- http://fancyerii.github.io/2019/06/30/xlnet-theory/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
