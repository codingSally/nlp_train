{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ELMo\n",
    "\n",
    "## 提出背景\n",
    "\n",
    "- 之前2013年提出的word2vec以及2014年提出的Glove, 在构建词向量时，将一个词生成对应的向量，无法解决一次多义的问题，而Elmo提出了一个很好的解决方案，不同于以往的一个词对应一个向量，是固定的，它预训练好的模型不再只是向量对应关系，而是一个预训练好的模型。\n",
    "\n",
    "- 使用时，将一句话或一段话输入模型，模型会根据上下文来推断每个词对应的向量\n",
    "\n",
    "- 这样的好处就是可以结合上下文语境来对多义词进行理解\n",
    "\n",
    "\n",
    "## 基本概念\n",
    "\n",
    "- Elmo的双向lstm语言模型\n",
    "\n",
    "- ELMo是一种新型深度语境化词表征，可对词进行复杂特征(如句法和语义)和词在语言语境中的变化进行建模(即对多义词进行建模)。我们的词向量是深度双向语言模型(biLM)内部状态的函数，在一个大型文本语料库中预训练而成。\n",
    "\n",
    "## 模型架构图\n",
    "\n",
    "![title](img/elmo1.png)\n",
    "\n",
    "- 其中，词向量的表示是基于当前的句子上下文\n",
    "\n",
    "- 高层LSTM, 用于捕捉上下文的词特征（语义）\n",
    "\n",
    "- 底层LSTM，用于捕捉句法层次信息（语法）\n",
    "\n",
    "## 基本原理\n",
    "\n",
    "![title](img/elmo2.png)\n",
    "\n",
    "\n",
    "## 模型优缺点\n",
    "\n",
    "### 优点\n",
    "\n",
    "- 解决了一词多义的问题\n",
    "\n",
    "- 在一些任务上效果有提升\n",
    "\n",
    "![title](img/elmo3.png)\n",
    "\n",
    "### 缺点\n",
    "\n",
    "- LSTM串行，训练成本大\n",
    "\n",
    "- LSTM对于长距离的特征提取，不如LSTM\n",
    "\n",
    "## 相关论文\n",
    "\n",
    "- https://arxiv.org/pdf/1802.05365.pdf\n",
    "\n",
    "\n",
    "## 相关博文\n",
    "\n",
    "- https://www.cnblogs.com/huangyc/p/9860430.html\n",
    "\n",
    "- https://zhuanlan.zhihu.com/p/51679783"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT\n",
    "\n",
    "- Generative Pre-Training\n",
    "\n",
    "## 基本原理\n",
    "\n",
    "- GPT模型分为两阶段，其核心思想是先通过无标签的文本去训练生成语言模型，再根据具体的NLP任务（如文本分类、机器翻译等），来通过有标签的数据对模型进行fine-tuning。\n",
    "\n",
    "- 具体来说，在这篇论文中提出了半监督的方法，即结合了无监督的预训练和有监督的fine-tuning。论文采用两阶段训练。首先，在未标记数据集上训练语言模型来学习神经网络模型的初始参数。随后，使用相应NLP任务中的有标签的数据地将这些参数微调，来适应当前任务。\n",
    "\n",
    "- GPT的预训练过程，其实和ELMO是类似的，主要不同在于两点：\n",
    "\n",
    "> 首先，特征抽取器不是用的RNN，而是用的Transformer，上面提到过它的特征抽取能力要强于RNN\n",
    "\n",
    "> 其次，GPT的预训练虽然仍然是以语言模型作为目标任务，但是采用的是单向的语言模型\n",
    "\n",
    "> - 所谓“单向”的含义是指：语言模型训练的任务目标是根据单词的上下文去正确预测单词Wi, Wj之前的单词序列Context-before称为上文，之后的单词序列Context-after称为下文。ELMO在做语言模型预训练的时候，预测单词Wi同时使用了上文和下文，而GPT则只采用Context-before这个单词的上文来进行预测，而抛开了下文\n",
    "\n",
    "- 模型的目标是学习一个通用的语言表示，可以经过很小的调整就应用到各种任务中\n",
    "\n",
    "- 这个模型的设置不需要目标任务和非标注的数据集在同一个领域\n",
    "\n",
    "## 模型结构\n",
    "\n",
    "![title](img/elmo4.png)\n",
    "\n",
    "## 模型原理\n",
    "\n",
    "![title](img/elmo5.png)\n",
    "\n",
    "![title](img/elmo6.png)\n",
    "\n",
    "\n",
    "## 具体任务的微调\n",
    "\n",
    "![title](img/elmo7.png)\n",
    "\n",
    "- GPT的缺点是，单向的语言模型，无法获取上下文相关的特征表示\n",
    "\n",
    "## 论文地址\n",
    "\n",
    "- https://www.cs.ubc.ca/~amuham01/LING530/papers/radford2018improving.pdf\n",
    "\n",
    "\n",
    "## 相关博文\n",
    "\n",
    "- https://zhuanlan.zhihu.com/p/59286975\n",
    "\n",
    "- https://blog.csdn.net/sinat_24330297/article/details/102501549"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT\n",
    "\n",
    "- Bidirectional Encoder Representations from Transformer\n",
    "\n",
    "- 基于语义理解的深度双向预训练Transformer\n",
    "\n",
    "## BERT的贡献\n",
    "\n",
    "- 证明了双向模型对文本特征表示的重要性\n",
    "\n",
    "- 证明了预训练模型能够消除很多繁重的任务相关的网络结构\n",
    "\n",
    "- 在11个NLP任务上，提升了state-of-art水平\n",
    "\n",
    "## 与其它词向量的关系\n",
    "\n",
    "- Word2vec等词向量是词维度，训练好就确定了\n",
    "\n",
    "- BERT句子维度的向量表示，依赖上下文构建结果\n",
    "\n",
    "- 词向量的建立在分布式假设前提下：即，相同上下文语境的词有相似的含义。那么如果两个同义词在不同的上线文中，得到的词向量就是不同的，因此无法处理同义词问题\n",
    "\n",
    "- 而ELMO、GPT、BERT，这些都是结合上下文的动态语言模型表征\n",
    "\n",
    "- 都是无监督训练，不需要标签信息\n",
    "\n",
    "## 背景知识\n",
    "\n",
    "![title](img/bert1.png)\n",
    "\n",
    "\n",
    "![title](img/bert2.png)\n",
    "\n",
    "\n",
    "## 基本概念\n",
    "\n",
    "- 要理解 Bert，5 个关键词帮助理解其思想，分别是 Pre-training、Deep、Bidirectional、Transformer、Language Understanding。\n",
    "\n",
    "> Pre-training: 论文作者认为，确实存在通用的语言模型，先用大量数据预训练一个通用模型，然后再微调模型，使其适用于下游任务。\n",
    "\n",
    "> Deep: Bert 与 Transformer 不同，Bert 的神经网络层更深，意味着它能更准确表达语义的理解，提取更多特征\n",
    "\n",
    "> Bidirectional: Bert 被设计成一个深度双向模型，使得神经网络更有效地从第一层本身一直到最后一层捕获来自目标词的左右上下文信息\n",
    "\n",
    "> Transformer: \n",
    "\n",
    "> - Bert 是基于 Tranformer 的深度双向语言表征模型，也就是利用 Transformer 结构构造了一个多层双向的Encoder 网络。它的特点之一就是所有层都联合上下文语境进行预训练。\n",
    "\n",
    "> - Bert 的目标是生成预训练语言模型，所以只需要 Encoder 机制。Transformer 的 Encoder 是一次性读取整个文本序列，而不是从左到右或者从右到左按顺序读取.\n",
    "\n",
    "> Language Understanding: Bert 是一个语言表征模型，能实现语言表征目标训练，通过深度双向 Transformer 模型达到语义理解的目的。\n",
    "\n",
    "## 模型架构\n",
    "\n",
    "![title](img/bert3.png)\n",
    "\n",
    "\n",
    "- 整体结构： Embedding + Transformer Encoder + Loss优化 \n",
    "\n",
    "## 基本原理\n",
    "\n",
    "- Bert整体分为两阶段，Pre-Train 和 Fine-tuning，其中Pre-Train阶段主要涉及 Embedding + Transformer Encoder以及针对Masked LM 和 NSP任务的Loss优化。而Fine-tuning阶段主要是通过具体下游任务，对模型参数进行微调。\n",
    "\n",
    " \n",
    "### Pre-Train阶段\n",
    "\n",
    "####  Embedding\n",
    "\n",
    "- Bert的输入相较于其他模型，采用了三个Embedding相加的方式，通过加入Token Embeddings、Segment Embedding，Position Embeddings三个向量，以此达到预训练和预测下一句的目的。\n",
    "\n",
    "![title](img/bert4.png)\n",
    "\n",
    "\n",
    "- Token Embeddings\n",
    "\n",
    "> 通过建立字向量表将每个字转换成一个一维向量，作为模型输入\n",
    "\n",
    "> 特别的，英文词汇会做更细粒度的切分，比如playing 或切割成 play 和 ##ing，中文目前尚未对输入文本进行分词，直接对单字构成的文本作为输入单位\n",
    "\n",
    "> <font color = 'red'> 将词切割成更细粒度的 Word Piece 是为了解决未登录词(OOV)的常见方法</font> \n",
    "\n",
    "> 举例说明\n",
    "\n",
    ">- 假如输入文本 ”I like dog“。下图则为 Token Embeddings 层实现过程。输入文本在送入 Token Embeddings 层之前要先进行 tokenization 处理，且两个特殊的 Token 会插入在文本开头 [CLS] 和结尾 [SEP]。（[CLS]是起始标记，[SEP]是句对分割标记）\n",
    "\n",
    ">- Bert 在处理英文文本时只需要 30522 个词，Token Embeddings 层会将每个词转换成 768 维向量，例子中 5 个Token 会被转换成一个 (6, 768) 的矩阵或 (1, 6, 768) 的张量。\n",
    "\n",
    "![title](img/bert5.png)\n",
    "\n",
    "\n",
    "- Segment Embedding\n",
    "\n",
    "> 主要用来区分不同的句子\n",
    "\n",
    "> - 例如：Bert 能够处理句子对的分类任务，这类任务就是判断两个文本是否是语义相似的。句子对中的两个句子被简单的拼接在一起后送入模型中，Bert 如何区分一个句子对是两个句子呢？答案就是 Segment Embeddings。\n",
    "\n",
    "> Segement Embeddings 层有两种向量表示，前一个向量是把 0 赋值给第一个句子的各个 Token，后一个向量是把1赋值给各个 Token，问答系统等任务要预测下一句，因此输入是有关联的句子。而文本分类只有一个句子，那么 Segement embeddings 就全部是 0。\n",
    "\n",
    "![title](img/bert6.png)\n",
    "\n",
    "\n",
    "- Position Embedding\n",
    "\n",
    "> 由于出现在文本不同位置的字/词所携带的语义信息存在差异(如 ”你爱我“ 和 ”我爱你“)，你和我虽然都和爱字很接近，但是位置不同，表示的含义不同,所以在Transformer中植入了Position Embedding\n",
    "\n",
    "> 在 RNN 中，第二个 ”I“ 和 第一个 ”I“ 表达的意义不一样，因为它们的隐状态不一样。对第二个 ”I“ 来说，隐状态经过 ”I think therefore“ 三个词，包含了前面三个词的信息，而第一个 ”I“ 只是一个初始值。因此，RNN 的隐状态保证在不同位置上相同的词有不同的输出向量表示。\n",
    "\n",
    "![title](img/bert7.png)\n",
    "\n",
    "\n",
    "> RNN能够让模型隐式的编码序列的顺序信息，相比之下，Transformer 的自注意力层 (Self-Attention) 对不同位置出现相同词给出的是同样的输出向量表示。尽管 Transformer 中两个 ”I“ 在不同的位置上，但是表示的向量是相同的。\n",
    "\n",
    "![title](img/bert8.png)\n",
    "\n",
    "> 所以，Transformer 中通过植入关于 Token 的相对位置或者绝对位置信息来表示序列的顺序信息。其中偶数位置，使用正弦编码，奇数位置，使用余弦编码。\n",
    "\n",
    "![title](img/bert9.png)\n",
    "\n",
    "> Bert 中处理的最长序列是 512 个 Token，长度超过 512 会被截取，Bert 在各个位置上学习一个向量来表示序列顺序的信息编码进来，这意味着 Position Embeddings 实际上是一个 (512, 768) 的 lookup 表，表第一行是代表第一个序列的每个位置，第二行代表序列第二个位置。\n",
    "\n",
    "> <font color = 'red'> 最后，Bert 模型将 Token Embeddings (1, n, 768) + Segment Embeddings(1, n, 768) + Position Embeddings(1, n, 768) 求和的方式得到一个 Embedding(1, n, 768) 作为模型的输入。(其中n表示单词的个数)</font>\n",
    "\n",
    "\n",
    "#### Transformer Encoder\n",
    "\n",
    "- Multi_Head Attention (多头注意力机制) + Feed Forward （前向传播）\n",
    "\n",
    "> - 多头注意力机制的应用\n",
    "\n",
    "![title](img/bert10.png)\n",
    "\n",
    "> - 前向传播的应用\n",
    "\n",
    "![title](img/bert11.png)\n",
    "\n",
    "####  Loss优化\n",
    "\n",
    "模型训练过程中，主要是训练Masked Language Model（Mask LM） 和 Next Sentence Prediction(NSP) 这两个任务，所以Loss优化也主要是优化这两个任务的损失\n",
    "\n",
    "- Masked Language Model（Mask ML）任务\n",
    "\n",
    "> 将输入句子‬mask（挖）掉一个词，然后通过上下文的词来预测这个词。\n",
    "\n",
    "> 通过预测的loss，计算Mask LM Loss,模型训练过程中，主要是最小化Mask LM Loss\n",
    "\n",
    "> 提出背景\n",
    "\n",
    "> - Maked LM 是为了解决单向信息问题，现有的语言模型的问题在于，没有同时利用双向信息，如 ELMO 号称是双向LM，但实际上是两个单向 RNN 构成的语言模型的拼接，由于时间序列的关系，RNN模型预测当前词只依赖前面出现过的词，对于后面的信息无从得知。\n",
    "\n",
    "> - 那么如何同时利用好前面的词和后面的词的语义呢？Bert 提出 Masked Language Model，也就是随机遮住句子中部分 Token，模型再去通过上下文语义去预测 Masked 的词，通过调整模型的参数使得模型预测正确率尽可能大\n",
    "\n",
    "> - 怎么理解这一逻辑，Bert 预训练过程就是模仿我们学习语言的过程，要准确的理解一个句子或一段文本的语义，就要学习上下文关系，从上下文语义来推测空缺单词的含义。而 Bert 的做法模拟了英语中的完形填空，随机将一些单词遮住，让 Bert 模型去预测这个单词，以此达到学习整个文本语义的目的。\n",
    "\n",
    "> 实现细节\n",
    "\n",
    "> - 随机MASK 15% 的word piece, 其中，这15%中，Mask的方式，又分为如下三种\n",
    "\n",
    "> - 80%选中的词用[MASK]代替\n",
    "\n",
    "> - 10%将选中的词用任意词代替\n",
    "\n",
    "> - 10%选中的词不发生变化\n",
    "\n",
    "> 这样做的目的是，尽量让模型知道每个词都有意义，除了要学习上下文信息，还需要提防每个词，因为每个词都不一定是对的。对于 Bert 来说，每个词都需要很好的理解和预测。从而避免模型仅学习到一种固定的模式。\n",
    "\n",
    "> - 具体解释详见下面博文\n",
    "\n",
    "- Next Sentence Prediction(NSP) 任务\n",
    "\n",
    "\n",
    "> 提出背景\n",
    "\n",
    "主要作用是得到问答，推理等句子对之间的关系\n",
    "\n",
    "> - 为了训练一个理解句子关系的模型，作者提出 Next Sentence Prediction，也即是预训练一个下一句预测的二分类任务，这个任务就是每次训练前都会从语料库中随机选择句子 A 和句子 B，50% 是正确的相邻的句子，50% 是随机选取的一个句子\n",
    "\n",
    "> - 直观来说，就是通过训练一个二分类，来判断两个句子是连续的还是不相关的\n",
    "\n",
    "> - 这个任务在预训练中能达到 97%-98% 的准确率，并且能很显著的提高 QA 和 NLI 的效果。\n",
    "\n",
    "> 实现细节\n",
    "\n",
    "![title](img/bert12.png)\n",
    "\n",
    "> - 在构建训练数据的过程中， 50％：B是跟随A的实际下一个句子（标记为IsNext）\n",
    "\n",
    "> - 50％：来自语料库的随机句子（标记为NotNext）\n",
    "\n",
    "\n",
    "- 因此整个模型通过优化这两个任务的损失，来训练整个模型。\n",
    "\n",
    "- 模型通过对 Masked LM 任务和 Next Sentence Prediction 任务进行联合训练，使模型输出的每个字 / 词的向量表示都能尽可能全面、准确地刻画输入文本（单句或语句对）的整体信息，为后续的微调任务提供更好的模型参数初始值。\n",
    "\n",
    "#### Bert预训练数据来源\n",
    "\n",
    "- 英文\n",
    "\n",
    "> BooksCorpus（800M words）\n",
    "\n",
    "> 英语维基百科（2,500M字）提取文本段落，忽略列表，表格和标题。\n",
    "\n",
    "\n",
    "- 中文\n",
    "\n",
    "> 中文维基百科\n",
    "\n",
    "### Fine-Tuning阶段\n",
    "\n",
    "- Fine-Tuning 就是指在已经训练好的语言模型基础上，使用有标签的数据对参数进行调整，使其更好的适用于下游任务。如对于分类问题在语言模型基础上加一层 softmax 网络，然后再新的预料上重新训练进行 Fine-Tuning。\n",
    "\n",
    "> Transformer中的自动注意机制允许BERT通过交换适当的输入和输出来模拟许多下游任务 \n",
    "\n",
    "> 将任务特定的输入和输出插入到BERT中，并对端到端的所有参数进行微调\n",
    "\n",
    "\n",
    "## Bert中每一层学到了什么\n",
    "\n",
    "- 在文章 ACL 2019：What does BERT learn about the structure of language? [https://hal.inria.fr/hal-02131630/document ] 中给出说明\n",
    "\n",
    "> 低层网络捕捉了短语级别的结构信息\n",
    "\n",
    "> 表层信息特征在底层网络（3，4）\n",
    "\n",
    "> 句法信息特征在中间层网络（6~9）\n",
    "\n",
    "> 语义信息特征在高层网络（9~12）\n",
    "\n",
    "> 主谓一致表现在中间层网络（8，9）\n",
    "\n",
    "## Bert的主要贡献\n",
    "\n",
    "- Bert 采用深度双向 Transformer 语言模型，通过 Mask LM 来达到训练深度双向预训练模型，较之前使用单向语言模型训练更准确，信息量更大，且语义理解更准确。\n",
    "\n",
    "- 论文表明，预训练模型能省去特定工程需要修改体系架构的麻烦，Bert 是第一个基于 fine-tune 的语言模型，它在大量句子级和 Token 级任务上展现了很好的性能。\n",
    "\n",
    "- Bert 的成功，一个重要原因就是数据量大，计算资源丰富。BERT 训练数据采用了英文的开源语料 BooksCropus以及英文维基百科数据，一共有 33 亿个词。同时 BERT 模型的标准版本有 1 亿的参数量，与 GPT 持平，而 BERT的大号版本有 3 亿多参数量，这应该是目前自然语言处理中最大的预训练模型了。\n",
    "\n",
    "\n",
    "## 源码地址\n",
    "\n",
    "- https://github.com/google-research/bert\n",
    "\n",
    "\n",
    "- https://github.com/hanxiao/bert-as-service\n",
    "\n",
    "\n",
    "## 论文地址\n",
    "\n",
    "- https://arxiv.org/pdf/1810.04805.pdf\n",
    "\n",
    "\n",
    "## 相关博文\n",
    "\n",
    "- https://zhuanlan.zhihu.com/p/50913043\n",
    "\n",
    "- https://www.jianshu.com/p/810ca25c4502\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 后续总结三大类预训练语言模型的对比"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "166.167px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
